{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03022c86",
   "metadata": {},
   "source": [
    "# Data Cleaning + 5-Chart EDA (Template)\n",
    "\n",
    "**Purpose:** Fast, standardized delivery for Fiverr clients.  \n",
    "**Inputs:** A CSV/XLSX file path.  \n",
    "**Outputs:** Cleaned dataset, 5 charts (PNGs), and a short insights summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a404fe-abce-488f-8ec5-bfe768d2b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Marketing Performance — Cleaner + Plots (Robust Dates) =====================\n",
    "# Raw columns expected (case-insensitive; spaces allowed and normalized):\n",
    "#   date, channel, spend, clicks, conversions, revenue\n",
    "#\n",
    "# Outputs:\n",
    "#   - marketing_clean.csv  (adds: cpc, cvr, roas)\n",
    "#   - charts/1_spend_by_channel.png\n",
    "#   - charts/2_roas_by_channel.png\n",
    "#   - charts/3_cvr_boxplot.png\n",
    "#   - charts/4_spend_timeseries.png\n",
    "#   - charts/5_revenue_vs_spend.png\n",
    "#\n",
    "# Requirements: Python 3.9+, pandas>=2.2, matplotlib>=3.9, numpy\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# ---------- I/O paths ----------\n",
    "RAW_PATH   = \"marketing_raw.csv\"\n",
    "CLEAN_PATH = \"marketing_clean.csv\"\n",
    "CHARTS_DIR = \"charts\"\n",
    "os.makedirs(CHARTS_DIR, exist_ok=True)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2b6ee15-9acf-4ccd-bd8d-b89e5ac05dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers used by plotting (improved) ----------\n",
    "def _savefig(out_dir: str, filename: str, fig=None, dpi: int = 160) -> str:\n",
    "    \"\"\"\n",
    "    Save current or provided Matplotlib figure to out_dir/filename and return the saved path.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "def _normalize_freq(freq: str) -> str:\n",
    "    \"\"\"Forward-compat mapping for pandas frequency aliases.\"\"\"\n",
    "    if not isinstance(freq, str):\n",
    "        return freq\n",
    "    freq = freq.upper()\n",
    "    mapping = {\n",
    "        \"M\": \"ME\",   # month-end → MonthEnd\n",
    "        \"MS\": \"MS\",  # month-start → MonthStart\n",
    "        \"Q\": \"QE\",   # quarter-end\n",
    "        \"QS\": \"QS\",  # quarter-start\n",
    "        \"Y\": \"YE\",   # year-end\n",
    "        \"A\": \"YE\",   # alias for annual end\n",
    "        \"AS\": \"YS\",  # year-start\n",
    "    }\n",
    "    return mapping.get(freq, freq)\n",
    "\n",
    "def _nice_time_axis(ax, idx, freq: str, max_ticks: int = 10, rotate: int = 0):\n",
    "    \"\"\"\n",
    "    Clean, compact date ticks by frequency.\n",
    "\n",
    "    - Accepts DatetimeIndex, PeriodIndex, numpy arrays, or lists of timestamps.\n",
    "    - Chooses readable tick positions (<= max_ticks).\n",
    "    - Custom quarter labeling when freq is QS/QE; YYYY-Qk.\n",
    "    \"\"\"\n",
    "    # Normalize/guard inputs\n",
    "    freq = _normalize_freq(freq)\n",
    "    # If index is not a DatetimeIndex, try to convert\n",
    "    if not isinstance(idx, pd.DatetimeIndex):\n",
    "        try:\n",
    "            idx = pd.DatetimeIndex(idx)\n",
    "        except Exception:\n",
    "            # Fall back to raw tick positions without date formatting\n",
    "            n = len(idx)\n",
    "            if n == 0:\n",
    "                return\n",
    "            tick_idx = np.arange(n) if n <= max_ticks else np.linspace(0, n - 1, max_ticks, dtype=int)\n",
    "            ax.set_xticks(tick_idx)\n",
    "            if rotate:\n",
    "                for t in ax.get_xticklabels():\n",
    "                    t.set_rotation(rotate)\n",
    "            ax.margins(x=0.01)\n",
    "            return\n",
    "\n",
    "    n = len(idx)\n",
    "    if n == 0:\n",
    "        return\n",
    "    tick_idx = np.arange(n) if n <= max_ticks else np.linspace(0, n - 1, max_ticks, dtype=int)\n",
    "    tick_pos = idx[tick_idx]\n",
    "    ax.set_xticks(tick_pos)\n",
    "\n",
    "    # Formatter selection\n",
    "    fmt = None\n",
    "    if freq == \"D\":\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "    elif freq == \"W\":\n",
    "        # ISO week label: YYYY-Www\n",
    "        fmt = mdates.DateFormatter(\"%G-W%V\")\n",
    "    elif freq in (\"MS\", \"ME\"):\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m\")\n",
    "    elif freq in (\"QS\", \"QE\"):\n",
    "        # Render custom quarter labels\n",
    "        labels = [f\"{d.year}-Q{((d.month - 1)//3) + 1}\" for d in tick_pos]\n",
    "        ax.set_xticklabels(labels)\n",
    "    else:\n",
    "        # Adaptive fallback\n",
    "        locator = mdates.AutoDateLocator(minticks=4, maxticks=max_ticks)\n",
    "        fmt = mdates.AutoDateFormatter(locator)\n",
    "        ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    if fmt is not None:\n",
    "        ax.xaxis.set_major_formatter(fmt)\n",
    "\n",
    "    if rotate:\n",
    "        for t in ax.get_xticklabels():\n",
    "            t.set_rotation(rotate)\n",
    "    ax.figure.autofmt_xdate()\n",
    "    ax.margins(x=0.01)\n",
    "\n",
    "def _has(df: pd.DataFrame, cols) -> bool:\n",
    "    \"\"\"Return True if all cols exist in df.columns.\"\"\"\n",
    "    return all(c in df.columns for c in cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f94277-fadf-4994-b697-3104fb5a98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Robust date parsing (with audit, improved) ----------\n",
    "_CANDIDATE_FORMATS = [\n",
    "    # ISO-like\n",
    "    \"%Y-%m-%d\",\n",
    "    \"%Y-%m-%d %H:%M\",\n",
    "    \"%Y-%m-%d %H:%M:%S\",\n",
    "    \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "    \"%Y-%m-%dT%H:%M\",\n",
    "    \"%Y-%m-%dT%H:%M:%S\",\n",
    "    \"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "    \"%Y-%m-%dT%H:%M%z\",\n",
    "    \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "    \"%Y-%m-%dT%H:%M:%S.%f%z\",\n",
    "    # Slash\n",
    "    \"%Y/%m/%d\",\n",
    "    \"%Y/%m/%d %H:%M\",\n",
    "    \"%Y/%m/%d %H:%M:%S\",\n",
    "    \"%d/%m/%Y\",\n",
    "    \"%d/%m/%Y %H:%M\",\n",
    "    \"%d/%m/%Y %H:%M:%S\",\n",
    "    \"%m/%d/%Y\",\n",
    "    \"%m/%d/%Y %H:%M\",\n",
    "    \"%m/%d/%Y %H:%M:%S\",\n",
    "    # Dash dayfirst/monthfirst\n",
    "    \"%d-%m-%Y\",\n",
    "    \"%d-%m-%Y %H:%M\",\n",
    "    \"%d-%m-%Y %H:%M:%S\",\n",
    "    \"%m-%d-%Y\",\n",
    "    \"%m-%d-%Y %H:%M\",\n",
    "    \"%m-%d-%Y %H:%M:%S\",\n",
    "    # Dots (EU)\n",
    "    \"%d.%m.%Y\",\n",
    "    \"%d.%m.%Y %H:%M\",\n",
    "    \"%d.%m.%Y %H:%M:%S\",\n",
    "    \"%Y.%m.%d\",\n",
    "    \"%Y.%m.%d %H:%M:%S\",\n",
    "]\n",
    "\n",
    "# Support s / ms / µs epoch lengths\n",
    "_EPOCH_10_RE = re.compile(r\"^\\s*\\d{10}\\s*$\")     # seconds\n",
    "_EPOCH_13_RE = re.compile(r\"^\\s*\\d{13}\\s*$\")     # milliseconds\n",
    "_EPOCH_16_RE = re.compile(r\"^\\s*\\d{16}\\s*$\")     # microseconds\n",
    "\n",
    "# Precise matcher: 'date', 'datetime', 'timestamp', 'time' as tokens, or *_at suffix (e.g., created_at)\n",
    "_DATE_NAME_RE = re.compile(r\"(?:^|_)(date|datetime|timestamp|time)(?:$|_)\", re.I)\n",
    "\n",
    "def _is_date_like_col(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    if _DATE_NAME_RE.search(n):\n",
    "        return True\n",
    "    if n.endswith(\"_at\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _guess_epoch_unit(sample: pd.Series, min_match: float = 0.7):\n",
    "    \"\"\"\n",
    "    Return 's', 'ms', or 'us' if >= min_match of non-null values look like UNIX epoch.\n",
    "    Works for object, integer, or float series.\n",
    "    \"\"\"\n",
    "    s = sample.dropna()\n",
    "    if s.empty:\n",
    "        return None\n",
    "    # Cast to string once\n",
    "    s = s.astype(str).str.strip()\n",
    "    n = len(s)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    if (s.str.match(_EPOCH_10_RE).sum() / n) >= min_match:\n",
    "        return \"s\"\n",
    "    if (s.str.match(_EPOCH_13_RE).sum() / n) >= min_match:\n",
    "        return \"ms\"\n",
    "    if (s.str.match(_EPOCH_16_RE).sum() / n) >= min_match:\n",
    "        return \"us\"\n",
    "    return None\n",
    "\n",
    "def _heuristic_filter_formats(strings: pd.Series) -> list:\n",
    "    \"\"\"\n",
    "    Narrow the candidate formats by simple separators/markers to reduce futile trials.\n",
    "    \"\"\"\n",
    "    sample = strings.dropna().astype(str)\n",
    "    if sample.empty:\n",
    "        return _CANDIDATE_FORMATS\n",
    "\n",
    "    text = \" \".join(sample.head(50))  # small peek\n",
    "    has_t = \"T\" in text\n",
    "    has_slash = \"/\" in text\n",
    "    has_dash = \"-\" in text\n",
    "    has_dot = \".\" in text\n",
    "    has_tz = \"+\" in text or \"Z\" in text\n",
    "\n",
    "    candidates = []\n",
    "    for fmt in _CANDIDATE_FORMATS:\n",
    "        if has_t and \"T\" not in fmt:\n",
    "            continue\n",
    "        if not has_t and \"T\" in fmt:\n",
    "            # Allow non-'T' formats when text doesn't contain 'T'\n",
    "            continue\n",
    "        if has_slash and \"/\" not in fmt:\n",
    "            continue\n",
    "        if has_dash and \"-\" not in fmt and \"T\" not in fmt:\n",
    "            continue\n",
    "        if has_dot and \".\" not in fmt and \"%f\" not in fmt:\n",
    "            # dots often imply European '.' or fractional seconds\n",
    "            pass  # don't over-filter; allow\n",
    "        if has_tz and \"%z\" not in fmt and \"T\" in fmt:\n",
    "            # if looks ISO-ish with timezone\n",
    "            pass  # allow; dateutil will handle fallback anyway\n",
    "        candidates.append(fmt)\n",
    "\n",
    "    # Fallback if we filtered too aggressively\n",
    "    if len(candidates) < 3:\n",
    "        return _CANDIDATE_FORMATS\n",
    "    return candidates\n",
    "\n",
    "def _guess_datetime_format(series: pd.Series, sample_size: int = 200, min_match: float = 0.7):\n",
    "    \"\"\"\n",
    "    Pick the strptime format that parses the highest fraction of a sample.\n",
    "    Uses heuristic pre-filtering of formats to speed up.\n",
    "    \"\"\"\n",
    "    s = series.dropna().astype(str)\n",
    "    if s.empty:\n",
    "        return None\n",
    "    if len(s) > sample_size:\n",
    "        s = s.sample(sample_size, random_state=0)\n",
    "\n",
    "    fmts = _heuristic_filter_formats(s)\n",
    "    best_fmt, best_score = None, 0.0\n",
    "    for fmt in fmts:\n",
    "        parsed = pd.to_datetime(s, format=fmt, errors=\"coerce\")\n",
    "        score = float(parsed.notna().mean())\n",
    "        if score > best_score:\n",
    "            best_fmt, best_score = fmt, score\n",
    "            if best_score == 1.0:  # perfect match, stop early\n",
    "                break\n",
    "    return best_fmt if best_score >= min_match else None\n",
    "\n",
    "def parse_date_with_audit(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    *,\n",
    "    dayfirst_fallback: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Parse df[col] via epoch → best-format → explicit timezone variants → general fallback.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parsed_series : pd.Series (dtype datetime64[ns])\n",
    "    audit_str     : str (how it was parsed)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If the series is already datetime64, returns it unchanged.\n",
    "    - If all values are null, returns all-NaT with a clear audit message.\n",
    "    - If formats are ambiguous, an optional dayfirst fallback is attempted.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Column '{col}' not found\")\n",
    "\n",
    "    s = df[col]\n",
    "    if s.dtype.kind in \"Mm\":\n",
    "        return s, f\"{col}: already datetime64[ns]\"\n",
    "\n",
    "    if s.isna().all():\n",
    "        return pd.to_datetime(s, errors=\"coerce\"), f\"{col}: all values null → all NaT\"\n",
    "\n",
    "    # 1) Epoch detection\n",
    "    unit = _guess_epoch_unit(s if s.dtype == \"O\" else s.astype(\"object\"))\n",
    "    if unit:\n",
    "        return pd.to_datetime(s, unit=unit, errors=\"coerce\"), f\"{col}: parsed as UNIX epoch ({unit})\"\n",
    "\n",
    "    # 2) Guess explicit strptime format\n",
    "    fmt = _guess_datetime_format(s)\n",
    "    if fmt:\n",
    "        return pd.to_datetime(s, format=fmt, errors=\"coerce\"), f\"{col}: parsed with explicit format '{fmt}'\"\n",
    "\n",
    "    # 3) Try explicit timezone-aware ISO if hints exist\n",
    "    s_str = s.astype(str)\n",
    "    if s_str.str.contains(\"Z|\\\\+\\\\d{2}:?\\\\d{2}\", regex=True).any():\n",
    "        for iso_fmt in (\"%Y-%m-%dT%H:%M%z\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%dT%H:%M:%S.%f%z\"):\n",
    "            parsed = pd.to_datetime(s, format=iso_fmt, errors=\"coerce\")\n",
    "            if parsed.notna().any():\n",
    "                return parsed, f\"{col}: parsed with explicit ISO+tZ format '{iso_fmt}'\"\n",
    "\n",
    "    # 4) General fallback (dateutil)\n",
    "    parsed = pd.to_datetime(s, errors=\"coerce\")\n",
    "    audit = f\"{col}: fallback via pd.to_datetime (dateutil parser)\"\n",
    "\n",
    "    # 5) Optional dayfirst retry for ambiguous d/m\n",
    "    if dayfirst_fallback and parsed.isna().mean() > 0 and s_str.str.contains(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\", regex=True).any():\n",
    "        parsed2 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "        # Prefer the parse that yields fewer NaT\n",
    "        if parsed2.notna().sum() > parsed.notna().sum():\n",
    "            parsed, audit = parsed2, f\"{col}: fallback via pd.to_datetime (dayfirst=True)\"\n",
    "\n",
    "    return parsed, audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f567c847-36a3-4950-911f-55ac386ed093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- GENERIC MISSING-VALUE HANDLER (two parameters, extended) ----------\n",
    "def handle_missing(col: str, action: str):\n",
    "    \"\"\"\n",
    "    Handle missing values for a single column using a compact action string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : str\n",
    "        Column name (silently no-ops if not present).\n",
    "    action : str\n",
    "        One of:\n",
    "          - 'set_zero' | 'zero'\n",
    "          - 'set_nan'  | 'nan'\n",
    "          - 'drop_row' | 'drop'                # drop rows where col is NA\n",
    "          - 'mean' | 'median' | 'mode'         # global fill\n",
    "          - 'mean_by:<groupcol>'\n",
    "          - 'median_by:<groupcol>'\n",
    "          - 'mode_by:<groupcol>'\n",
    "          - 'percentile:<q>'                   # e.g., 'percentile:0.5' (=median)\n",
    "          - 'const:<value>'                    # e.g., 'const:0', 'const:(missing)'\n",
    "          - 'ffill' | 'bfill'\n",
    "          - 'interpolate:<method>'             # 'linear' (default), 'time', 'polynomial-2', ...\n",
    "    Notes\n",
    "    -----\n",
    "    - Numeric-only ops (mean/median/percentile/interpolate) are applied only if dtype is numeric.\n",
    "    - For 'interpolate:time', a DatetimeIndex is expected; falls back to 'linear' if absent.\n",
    "    - 'mode' works for both numeric/object. If multiple modes, first is used.\n",
    "    \"\"\"\n",
    "    global df\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "\n",
    "    a = action.strip()\n",
    "\n",
    "    # Helpers\n",
    "    def _is_num(s): return pd.api.types.is_numeric_dtype(s)\n",
    "    def _const_value(raw, series):\n",
    "        # Try to coerce const to column dtype when possible\n",
    "        raw = raw.strip()\n",
    "        if raw.lower() in {\"nan\", \"na\", \"null\"}:\n",
    "            return np.nan\n",
    "        if _is_num(series):\n",
    "            try:\n",
    "                return float(raw)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # For datetimes, try parsing\n",
    "        if pd.api.types.is_datetime64_any_dtype(series):\n",
    "            try:\n",
    "                return pd.to_datetime(raw, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return raw  # fallback: string\n",
    "\n",
    "    # Basic actions\n",
    "    a_low = a.lower()\n",
    "    if a_low in {\"set_zero\", \"zero\"}:\n",
    "        df[col] = df[col].fillna(0)\n",
    "        return\n",
    "    if a_low in {\"set_nan\", \"nan\"}:\n",
    "        df[col] = df[col].where(~df[col].isna(), np.nan)\n",
    "        return\n",
    "    if a_low in {\"drop_row\", \"drop\"}:\n",
    "        df = df[~df[col].isna()]\n",
    "        return\n",
    "    if a_low == \"mean\":\n",
    "        if _is_num(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        return\n",
    "    if a_low == \"median\":\n",
    "        if _is_num(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        return\n",
    "    if a_low == \"mode\":\n",
    "        if df[col].isna().any():\n",
    "            m = df[col].mode(dropna=True)\n",
    "            if not m.empty:\n",
    "                df[col] = df[col].fillna(m.iloc[0])\n",
    "        return\n",
    "    if a_low in {\"ffill\", \"bfill\"}:\n",
    "        df[col] = df[col].fillna(method=a_low)\n",
    "        return\n",
    "\n",
    "    # Parameterized: const:<value>\n",
    "    if a_low.startswith(\"const:\"):\n",
    "        raw = a.split(\":\", 1)[1]\n",
    "        val = _const_value(raw, df[col])\n",
    "        df[col] = df[col].fillna(val)\n",
    "        return\n",
    "\n",
    "    # Parameterized: percentile:<q>\n",
    "    if a_low.startswith(\"percentile:\"):\n",
    "        if _is_num(df[col]):\n",
    "            try:\n",
    "                q = float(a.split(\":\", 1)[1])\n",
    "                q = min(max(q, 0.0), 1.0)\n",
    "                val = df[col].quantile(q)\n",
    "                df[col] = df[col].fillna(val)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    # Parameterized: mean_by:/median_by:/mode_by:<groupcol>\n",
    "    if any(a_low.startswith(prefix) for prefix in (\"mean_by:\", \"median_by:\", \"mode_by:\")):\n",
    "        try:\n",
    "            method, grp = a_low.split(\":\", 1)\n",
    "            grp = grp.strip()\n",
    "            if grp in df.columns and df[col].isna().any():\n",
    "                if method == \"mean_by\" and _is_num(df[col]):\n",
    "                    fill_vals = df.groupby(grp)[col].transform(\"mean\")\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "                elif method == \"median_by\" and _is_num(df[col]):\n",
    "                    fill_vals = df.groupby(grp)[col].transform(\"median\")\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "                elif method == \"mode_by\":\n",
    "                    # Works for numeric or object\n",
    "                    def _grp_mode(s):\n",
    "                        m = s.mode(dropna=True)\n",
    "                        return m.iloc[0] if not m.empty else np.nan\n",
    "                    fill_vals = df.groupby(grp)[col].transform(_grp_mode)\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    # Parameterized: interpolate:<method>\n",
    "    if a_low.startswith(\"interpolate:\"):\n",
    "        method = a.split(\":\", 1)[1].strip()\n",
    "        if not method:\n",
    "            method = \"linear\"\n",
    "        if _is_num(df[col]):\n",
    "            try:\n",
    "                if method == \"time\" and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                    # Fallback if no DatetimeIndex\n",
    "                    method = \"linear\"\n",
    "                df[col] = df[col].interpolate(method=method, limit_direction=\"both\")\n",
    "            except Exception:\n",
    "                # Silent fallback: leave as is\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    # Unknown action -> no-op\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6000b9dc-9517-4a3d-b865-04e2747d2933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Date parsing audit]\n",
      " - date: parsed with explicit format '%Y-%m-%d'\n",
      "Saved cleaned dataset to marketing_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# ===================== 1) LOAD + CLEAN =====================\n",
    "df_raw = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# Normalize column names (lower + underscores) to be tolerant to raw headers\n",
    "df_raw.columns = [re.sub(r\"\\s+\", \"_\", c.strip().lower()) for c in df_raw.columns]\n",
    "\n",
    "required = [\"date\", \"channel\", \"spend\", \"clicks\", \"conversions\", \"revenue\"]\n",
    "if not _has(df_raw, required):\n",
    "    missing = sorted(list(set(required) - set(df_raw.columns)))\n",
    "    raise ValueError(f\"Missing required columns in RAW: {missing}\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Trim strings in object cols\n",
    "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# Normalize obvious placeholders → NaN up-front\n",
    "df = df.replace(\n",
    "    to_replace=[\"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\", \"NaN\", \"nan\", \"NULL\", \"null\", \"-\"],\n",
    "    value=np.nan\n",
    ")\n",
    "\n",
    "# Canonicalize channel names\n",
    "df[\"channel\"] = df[\"channel\"].astype(\"string\").str.strip().str.title()\n",
    "\n",
    "# Robust date parsing (with audit) via helper (avoids deprecated inference path)\n",
    "parsed_date, date_audit = parse_date_with_audit(df, \"date\")\n",
    "df[\"date\"] = parsed_date\n",
    "print(\"[Date parsing audit]\")\n",
    "print(\" -\", date_audit)\n",
    "\n",
    "# Numeric coercions (explicit; no deprecated flags)\n",
    "for c in [\"spend\", \"clicks\", \"conversions\", \"revenue\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Enforce intended missing-policy via generic handler\n",
    "# (Keep channel missing as real NaN; do not mode-fill.)\n",
    "handle_missing(\"channel\", \"set_nan\")\n",
    "\n",
    "# Drop rows lacking key identifiers (date or channel)\n",
    "df = df[df[\"date\"].notna() & df[\"channel\"].notna()]\n",
    "\n",
    "# Derived metrics (NaN if denominator <= 0)\n",
    "df[\"cpc\"]  = np.where(df[\"clicks\"] > 0, df[\"spend\"] / df[\"clicks\"], np.nan)\n",
    "df[\"cvr\"]  = np.where(df[\"clicks\"] > 0, df[\"conversions\"] / df[\"clicks\"], np.nan)\n",
    "df[\"roas\"] = np.where(df[\"spend\"]  > 0, df[\"revenue\"] / df[\"spend\"],  np.nan)\n",
    "\n",
    "# De-duplicate\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save clean file (write visible NaNs so Excel doesn’t show blanks)\n",
    "df.to_csv(CLEAN_PATH, index=False, na_rep=\"NaN\")\n",
    "print(f\"Saved cleaned dataset to {CLEAN_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d79aad5-6d14-45a1-a010-812ef9776a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight theme helpers (do not change your existing helpers like save_chart / require_cols / nice_time_axis)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "# Cohesive palette & rcParams that won't break your logic\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"axes.titleweight\": \"semibold\",\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"grid.alpha\": 0.3,\n",
    "})\n",
    "\n",
    "_PALETTE = [\n",
    "    \"#4e79a7\", \"#f28e2b\", \"#e15759\", \"#76b7b2\", \"#59a14f\",\n",
    "    \"#edc949\", \"#af7aa1\", \"#ff9da7\", \"#9c755f\", \"#bab0ab\",\n",
    "]\n",
    "\n",
    "def _colors(n: int):\n",
    "    base = list(_PALETTE)\n",
    "    return (base * ((n + len(base) - 1) // len(base)))[:n]\n",
    "\n",
    "\n",
    "def _apply_theme(ax):\n",
    "    ax.set_facecolor(\"#f9fafb\")\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "    ax.set_axisbelow(True)\n",
    "    for spine in (\"top\", \"right\"):\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "\n",
    "# ===================== 2) PLOTTING FUNCTIONS =====================\n",
    "\n",
    "def plot_total_spend_by_channel(df: pd.DataFrame, outfile: str = \"1_spend_by_channel.png\"):\n",
    "    \"\"\"Bar: total spend by channel.\"\"\"\n",
    "    if not require_cols(df, [\"channel\", \"spend\"]):\n",
    "        return\n",
    "    s = (df.groupby(\"channel\", dropna=False)[\"spend\"].sum().sort_values(ascending=False))\n",
    "    fig, ax = plt.subplots(figsize=(12, 5.2))\n",
    "    colors = _colors(len(s))\n",
    "    s.plot(kind=\"bar\", ax=ax, color=colors, edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"Total Spend by Channel\")\n",
    "    ax.set_xlabel(\"Channel\"); ax.set_ylabel(\"Spend\")\n",
    "    _apply_theme(ax)\n",
    "    save_chart(fig, outfile)\n",
    "\n",
    "\n",
    "def plot_avg_roas_by_channel(df: pd.DataFrame, outfile: str = \"2_roas_by_channel.png\"):\n",
    "    \"\"\"Bar: mean ROAS by channel.\"\"\"\n",
    "    if not require_cols(df, [\"channel\", \"roas\"]):\n",
    "        return\n",
    "    s = (df.dropna(subset=[\"roas\"])\\\n",
    "           .groupby(\"channel\", dropna=False)[\"roas\"].mean()\\\n",
    "           .sort_values(ascending=False))\n",
    "    fig, ax = plt.subplots(figsize=(12, 5.2))\n",
    "    colors = _colors(len(s))\n",
    "    s.plot(kind=\"bar\", ax=ax, color=colors, edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"Average ROAS by Channel\")\n",
    "    ax.set_xlabel(\"Channel\"); ax.set_ylabel(\"ROAS\")\n",
    "    _apply_theme(ax)\n",
    "    save_chart(fig, outfile)\n",
    "\n",
    "\n",
    "def plot_cvr_boxplot(df: pd.DataFrame, outfile: str = \"3_cvr_boxplot.png\"):\n",
    "    \"\"\"Boxplot: CVR distribution by channel (uses Matplotlib 3.9+ tick_labels).\"\"\"\n",
    "    if not require_cols(df, [\"channel\", \"cvr\"]):\n",
    "        return\n",
    "    d = df.dropna(subset=[\"cvr\"])\n",
    "    if d.empty:\n",
    "        return\n",
    "    order = d.groupby(\"channel\")[\"cvr\"].median().sort_values(ascending=False).index\n",
    "    data = [d.loc[d[\"channel\"] == ch, \"cvr\"].to_numpy(dtype=float) for ch in order]\n",
    "    fig, ax = plt.subplots(figsize=(12, 5.5))\n",
    "    bp = ax.boxplot(data, patch_artist=True, tick_labels=order)  # no deprecated 'labels'\n",
    "    # Color each box subtly\n",
    "    cols = _colors(len(order))\n",
    "    for i, box in enumerate(bp[\"boxes\"]):\n",
    "        box.set(facecolor=cols[i], edgecolor=\"#ffffff\", linewidth=0.8, alpha=0.95)\n",
    "    for item in bp[\"whiskers\"] + bp[\"caps\"]:\n",
    "        item.set(color=\"#666666\", linewidth=0.8)\n",
    "    for med in bp[\"medians\"]:\n",
    "        med.set(color=\"#1f1f1f\", linewidth=1.4)\n",
    "    ax.set_title(\"CVR by Channel\")\n",
    "    ax.set_xlabel(\"Channel\"); ax.set_ylabel(\"CVR\")\n",
    "    _apply_theme(ax)\n",
    "    save_chart(fig, outfile)\n",
    "\n",
    "\n",
    "def plot_spend_timeseries_by_channel(df: pd.DataFrame, outfile: str = \"4_spend_timeseries.png\",\n",
    "                                     last_n_days: int | None = 60):\n",
    "    \"\"\"Line: spend per day by channel (optionally last N days).\"\"\"\n",
    "    if not require_cols(df, [\"date\", \"channel\", \"spend\"]):\n",
    "        return\n",
    "    d = df.copy()\n",
    "    if last_n_days is not None:\n",
    "        cutoff = d[\"date\"].max() - pd.Timedelta(days=last_n_days - 1)\n",
    "        d = d[d[\"date\"] >= cutoff]\n",
    "    ts = (d.groupby([\"date\", \"channel\"], as_index=False)[\"spend\"].sum()\n",
    "            .pivot(index=\"date\", columns=\"channel\", values=\"spend\")\n",
    "            .sort_index())\n",
    "    fig, ax = plt.subplots(figsize=(12, 5.5))\n",
    "    cols = _colors(len(ts.columns))\n",
    "    for i, ch in enumerate(ts.columns):\n",
    "        ax.plot(ts.index, ts[ch], label=ch, linewidth=2.0, marker=\"o\", markersize=3.2, color=cols[i])\n",
    "    ax.set_title(f\"Spend by Channel{' (Last ' + str(last_n_days) + ' Days)' if last_n_days else ''}\")\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Spend\")\n",
    "    ax.legend(ncol=3, frameon=False)\n",
    "    nice_time_axis(ax, ts.index.to_numpy(), max_ticks=9, rotate=0)\n",
    "    _apply_theme(ax)\n",
    "    save_chart(fig, outfile)\n",
    "\n",
    "\n",
    "def plot_revenue_vs_spend_scatter(df: pd.DataFrame, outfile: str = \"5_revenue_vs_spend.png\"):\n",
    "    \"\"\"Scatter: total revenue vs total spend by channel with annotations.\"\"\"\n",
    "    if not require_cols(df, [\"channel\", \"spend\", \"revenue\"]):\n",
    "        return\n",
    "    agg = df.groupby(\"channel\", dropna=False).agg(spend=(\"spend\", \"sum\"),\n",
    "                                                  revenue=(\"revenue\", \"sum\")).reset_index()\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    ax.scatter(agg[\"spend\"], agg[\"revenue\"], marker=\"o\", s=80, alpha=0.9,\n",
    "               edgecolors=\"#ffffff\", linewidths=0.6, color=\"#4e79a7\")\n",
    "    for _, r in agg.iterrows():\n",
    "        ax.annotate(str(r[\"channel\"]), (r[\"spend\"], r[\"revenue\"]),\n",
    "                    xytext=(6, 4), textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.15\", fc=\"#ffffff\", ec=\"none\", alpha=0.75))\n",
    "    ax.set_title(\"Revenue vs Spend by Channel\")\n",
    "    ax.set_xlabel(\"Spend\"); ax.set_ylabel(\"Revenue\")\n",
    "    _apply_theme(ax)\n",
    "    save_chart(fig, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88bf876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 3) RUN — with guards =====================\n",
    "if require_cols(df, [\"channel\", \"spend\"]):\n",
    "    plot_total_spend_by_channel(df, outfile=\"1_spend_by_channel.png\")\n",
    "\n",
    "if require_cols(df, [\"channel\", \"roas\"]):\n",
    "    plot_avg_roas_by_channel(df, outfile=\"2_roas_by_channel.png\")\n",
    "\n",
    "if require_cols(df, [\"channel\", \"cvr\"]):\n",
    "    plot_cvr_boxplot(df, outfile=\"3_cvr_boxplot.png\")\n",
    "\n",
    "if require_cols(df, [\"date\", \"channel\", \"spend\"]):\n",
    "    plot_spend_timeseries_by_channel(df, outfile=\"4_spend_timeseries.png\", last_n_days=60)\n",
    "\n",
    "if require_cols(df, [\"channel\", \"spend\", \"revenue\"]):\n",
    "    plot_revenue_vs_spend_scatter(df, outfile=\"5_revenue_vs_spend.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cfe65",
   "metadata": {},
   "source": [
    "\n",
    "### Delivery Notes (copy into your PDF summary)\n",
    "- **Data health**: duplicates removed; numeric nulls filled with median (customize as needed).\n",
    "- **Key patterns**: (write 3–5 bullet points referencing the charts)\n",
    "- **Outliers/Warnings**: (mention anomalies, missing ranges, unexpected spikes)\n",
    "- **Next steps**: (what simple analysis/modeling could help their decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8d39a-57fc-4400-9a1c-c81a218a91b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
