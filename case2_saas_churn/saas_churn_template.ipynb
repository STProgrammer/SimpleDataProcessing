{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03022c86",
   "metadata": {},
   "source": [
    "# Data Cleaning + 5-Chart EDA (Template)\n",
    "\n",
    "**Purpose:** Fast, standardized delivery for Fiverr clients.  \n",
    "**Inputs:** A CSV/XLSX file path.  \n",
    "**Outputs:** Cleaned dataset, 5 charts (PNGs), and a short insights summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe193e5f-10de-4183-844d-76d09f55414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== SaaS Churn — Load + Clean (Robust Dates) =====================\n",
    "# Raw columns expected (case-sensitive):\n",
    "# user_id, signup_date, plan, country, monthly_churn_prob, churn_month, arpu\n",
    "# (raw has NO 'churned'; we derive it)\n",
    "#\n",
    "# Outputs:\n",
    "# - saas_clean.csv\n",
    "# - charts/*.png (generated elsewhere)\n",
    "\n",
    "import os, re, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# ---------- Paths ----------\n",
    "RAW_PATH   = \"saas_raw.csv\"\n",
    "CLEAN_PATH = \"saas_clean.csv\"\n",
    "CHARTS_DIR = \"charts\"\n",
    "os.makedirs(CHARTS_DIR, exist_ok=True)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568fe852-8ba3-4851-b056-590a5d168d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers used by plotting (improved) ----------\n",
    "def _savefig(out_dir: str, filename: str, fig=None, dpi: int = 160) -> str:\n",
    "    \"\"\"\n",
    "    Save current or provided Matplotlib figure to out_dir/filename and return the saved path.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "def _normalize_freq(freq: str) -> str:\n",
    "    \"\"\"Forward-compat mapping for pandas frequency aliases.\"\"\"\n",
    "    if not isinstance(freq, str):\n",
    "        return freq\n",
    "    freq = freq.upper()\n",
    "    mapping = {\n",
    "        \"M\": \"ME\",   # month-end → MonthEnd\n",
    "        \"MS\": \"MS\",  # month-start → MonthStart\n",
    "        \"Q\": \"QE\",   # quarter-end\n",
    "        \"QS\": \"QS\",  # quarter-start\n",
    "        \"Y\": \"YE\",   # year-end\n",
    "        \"A\": \"YE\",   # alias for annual end\n",
    "        \"AS\": \"YS\",  # year-start\n",
    "    }\n",
    "    return mapping.get(freq, freq)\n",
    "\n",
    "def _nice_time_axis(ax, idx, freq: str, max_ticks: int = 10, rotate: int = 0):\n",
    "    \"\"\"\n",
    "    Clean, compact date ticks by frequency.\n",
    "\n",
    "    - Accepts DatetimeIndex, PeriodIndex, numpy arrays, or lists of timestamps.\n",
    "    - Chooses readable tick positions (<= max_ticks).\n",
    "    - Custom quarter labeling when freq is QS/QE; YYYY-Qk.\n",
    "    \"\"\"\n",
    "    # Normalize/guard inputs\n",
    "    freq = _normalize_freq(freq)\n",
    "    # If index is not a DatetimeIndex, try to convert\n",
    "    if not isinstance(idx, pd.DatetimeIndex):\n",
    "        try:\n",
    "            idx = pd.DatetimeIndex(idx)\n",
    "        except Exception:\n",
    "            # Fall back to raw tick positions without date formatting\n",
    "            n = len(idx)\n",
    "            if n == 0:\n",
    "                return\n",
    "            tick_idx = np.arange(n) if n <= max_ticks else np.linspace(0, n - 1, max_ticks, dtype=int)\n",
    "            ax.set_xticks(tick_idx)\n",
    "            if rotate:\n",
    "                for t in ax.get_xticklabels():\n",
    "                    t.set_rotation(rotate)\n",
    "            ax.margins(x=0.01)\n",
    "            return\n",
    "\n",
    "    n = len(idx)\n",
    "    if n == 0:\n",
    "        return\n",
    "    tick_idx = np.arange(n) if n <= max_ticks else np.linspace(0, n - 1, max_ticks, dtype=int)\n",
    "    tick_pos = idx[tick_idx]\n",
    "    ax.set_xticks(tick_pos)\n",
    "\n",
    "    # Formatter selection\n",
    "    fmt = None\n",
    "    if freq == \"D\":\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "    elif freq == \"W\":\n",
    "        # ISO week label: YYYY-Www\n",
    "        fmt = mdates.DateFormatter(\"%G-W%V\")\n",
    "    elif freq in (\"MS\", \"ME\"):\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m\")\n",
    "    elif freq in (\"QS\", \"QE\"):\n",
    "        # Render custom quarter labels\n",
    "        labels = [f\"{d.year}-Q{((d.month - 1)//3) + 1}\" for d in tick_pos]\n",
    "        ax.set_xticklabels(labels)\n",
    "    else:\n",
    "        # Adaptive fallback\n",
    "        locator = mdates.AutoDateLocator(minticks=4, maxticks=max_ticks)\n",
    "        fmt = mdates.AutoDateFormatter(locator)\n",
    "        ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    if fmt is not None:\n",
    "        ax.xaxis.set_major_formatter(fmt)\n",
    "\n",
    "    if rotate:\n",
    "        for t in ax.get_xticklabels():\n",
    "            t.set_rotation(rotate)\n",
    "    ax.figure.autofmt_xdate()\n",
    "    ax.margins(x=0.01)\n",
    "\n",
    "def _has(df: pd.DataFrame, cols) -> bool:\n",
    "    \"\"\"Return True if all cols exist in df.columns.\"\"\"\n",
    "    return all(c in df.columns for c in cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94db3f41-6d6f-41f2-a0aa-2d176dbdf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Robust date parsing (with audit, improved) ----------\n",
    "_CANDIDATE_FORMATS = [\n",
    "    # ISO-like\n",
    "    \"%Y-%m-%d\",\n",
    "    \"%Y-%m-%d %H:%M\",\n",
    "    \"%Y-%m-%d %H:%M:%S\",\n",
    "    \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "    \"%Y-%m-%dT%H:%M\",\n",
    "    \"%Y-%m-%dT%H:%M:%S\",\n",
    "    \"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "    \"%Y-%m-%dT%H:%M%z\",\n",
    "    \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "    \"%Y-%m-%dT%H:%M:%S.%f%z\",\n",
    "    # Slash\n",
    "    \"%Y/%m/%d\",\n",
    "    \"%Y/%m/%d %H:%M\",\n",
    "    \"%Y/%m/%d %H:%M:%S\",\n",
    "    \"%d/%m/%Y\",\n",
    "    \"%d/%m/%Y %H:%M\",\n",
    "    \"%d/%m/%Y %H:%M:%S\",\n",
    "    \"%m/%d/%Y\",\n",
    "    \"%m/%d/%Y %H:%M\",\n",
    "    \"%m/%d/%Y %H:%M:%S\",\n",
    "    # Dash dayfirst/monthfirst\n",
    "    \"%d-%m-%Y\",\n",
    "    \"%d-%m-%Y %H:%M\",\n",
    "    \"%d-%m-%Y %H:%M:%S\",\n",
    "    \"%m-%d-%Y\",\n",
    "    \"%m-%d-%Y %H:%M\",\n",
    "    \"%m-%d-%Y %H:%M:%S\",\n",
    "    # Dots (EU)\n",
    "    \"%d.%m.%Y\",\n",
    "    \"%d.%m.%Y %H:%M\",\n",
    "    \"%d.%m.%Y %H:%M:%S\",\n",
    "    \"%Y.%m.%d\",\n",
    "    \"%Y.%m.%d %H:%M:%S\",\n",
    "]\n",
    "\n",
    "# Support s / ms / µs epoch lengths\n",
    "_EPOCH_10_RE = re.compile(r\"^\\s*\\d{10}\\s*$\")     # seconds\n",
    "_EPOCH_13_RE = re.compile(r\"^\\s*\\d{13}\\s*$\")     # milliseconds\n",
    "_EPOCH_16_RE = re.compile(r\"^\\s*\\d{16}\\s*$\")     # microseconds\n",
    "\n",
    "# Precise matcher: 'date', 'datetime', 'timestamp', 'time' as tokens, or *_at suffix (e.g., created_at)\n",
    "_DATE_NAME_RE = re.compile(r\"(?:^|_)(date|datetime|timestamp|time)(?:$|_)\", re.I)\n",
    "\n",
    "def _is_date_like_col(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    if _DATE_NAME_RE.search(n):\n",
    "        return True\n",
    "    if n.endswith(\"_at\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _guess_epoch_unit(sample: pd.Series, min_match: float = 0.7):\n",
    "    \"\"\"\n",
    "    Return 's', 'ms', or 'us' if >= min_match of non-null values look like UNIX epoch.\n",
    "    Works for object, integer, or float series.\n",
    "    \"\"\"\n",
    "    s = sample.dropna()\n",
    "    if s.empty:\n",
    "        return None\n",
    "    # Cast to string once\n",
    "    s = s.astype(str).str.strip()\n",
    "    n = len(s)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    if (s.str.match(_EPOCH_10_RE).sum() / n) >= min_match:\n",
    "        return \"s\"\n",
    "    if (s.str.match(_EPOCH_13_RE).sum() / n) >= min_match:\n",
    "        return \"ms\"\n",
    "    if (s.str.match(_EPOCH_16_RE).sum() / n) >= min_match:\n",
    "        return \"us\"\n",
    "    return None\n",
    "\n",
    "def _heuristic_filter_formats(strings: pd.Series) -> list:\n",
    "    \"\"\"\n",
    "    Narrow the candidate formats by simple separators/markers to reduce futile trials.\n",
    "    \"\"\"\n",
    "    sample = strings.dropna().astype(str)\n",
    "    if sample.empty:\n",
    "        return _CANDIDATE_FORMATS\n",
    "\n",
    "    text = \" \".join(sample.head(50))  # small peek\n",
    "    has_t = \"T\" in text\n",
    "    has_slash = \"/\" in text\n",
    "    has_dash = \"-\" in text\n",
    "    has_dot = \".\" in text\n",
    "    has_tz = \"+\" in text or \"Z\" in text\n",
    "\n",
    "    candidates = []\n",
    "    for fmt in _CANDIDATE_FORMATS:\n",
    "        if has_t and \"T\" not in fmt:\n",
    "            continue\n",
    "        if not has_t and \"T\" in fmt:\n",
    "            # Allow non-'T' formats when text doesn't contain 'T'\n",
    "            continue\n",
    "        if has_slash and \"/\" not in fmt:\n",
    "            continue\n",
    "        if has_dash and \"-\" not in fmt and \"T\" not in fmt:\n",
    "            continue\n",
    "        if has_dot and \".\" not in fmt and \"%f\" not in fmt:\n",
    "            # dots often imply European '.' or fractional seconds\n",
    "            pass  # don't over-filter; allow\n",
    "        if has_tz and \"%z\" not in fmt and \"T\" in fmt:\n",
    "            # if looks ISO-ish with timezone\n",
    "            pass  # allow; dateutil will handle fallback anyway\n",
    "        candidates.append(fmt)\n",
    "\n",
    "    # Fallback if we filtered too aggressively\n",
    "    if len(candidates) < 3:\n",
    "        return _CANDIDATE_FORMATS\n",
    "    return candidates\n",
    "\n",
    "def _guess_datetime_format(series: pd.Series, sample_size: int = 200, min_match: float = 0.7):\n",
    "    \"\"\"\n",
    "    Pick the strptime format that parses the highest fraction of a sample.\n",
    "    Uses heuristic pre-filtering of formats to speed up.\n",
    "    \"\"\"\n",
    "    s = series.dropna().astype(str)\n",
    "    if s.empty:\n",
    "        return None\n",
    "    if len(s) > sample_size:\n",
    "        s = s.sample(sample_size, random_state=0)\n",
    "\n",
    "    fmts = _heuristic_filter_formats(s)\n",
    "    best_fmt, best_score = None, 0.0\n",
    "    for fmt in fmts:\n",
    "        parsed = pd.to_datetime(s, format=fmt, errors=\"coerce\")\n",
    "        score = float(parsed.notna().mean())\n",
    "        if score > best_score:\n",
    "            best_fmt, best_score = fmt, score\n",
    "            if best_score == 1.0:  # perfect match, stop early\n",
    "                break\n",
    "    return best_fmt if best_score >= min_match else None\n",
    "\n",
    "def parse_date_with_audit(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    *,\n",
    "    dayfirst_fallback: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Parse df[col] via epoch → best-format → explicit timezone variants → general fallback.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parsed_series : pd.Series (dtype datetime64[ns])\n",
    "    audit_str     : str (how it was parsed)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If the series is already datetime64, returns it unchanged.\n",
    "    - If all values are null, returns all-NaT with a clear audit message.\n",
    "    - If formats are ambiguous, an optional dayfirst fallback is attempted.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Column '{col}' not found\")\n",
    "\n",
    "    s = df[col]\n",
    "    if s.dtype.kind in \"Mm\":\n",
    "        return s, f\"{col}: already datetime64[ns]\"\n",
    "\n",
    "    if s.isna().all():\n",
    "        return pd.to_datetime(s, errors=\"coerce\"), f\"{col}: all values null → all NaT\"\n",
    "\n",
    "    # 1) Epoch detection\n",
    "    unit = _guess_epoch_unit(s if s.dtype == \"O\" else s.astype(\"object\"))\n",
    "    if unit:\n",
    "        return pd.to_datetime(s, unit=unit, errors=\"coerce\"), f\"{col}: parsed as UNIX epoch ({unit})\"\n",
    "\n",
    "    # 2) Guess explicit strptime format\n",
    "    fmt = _guess_datetime_format(s)\n",
    "    if fmt:\n",
    "        return pd.to_datetime(s, format=fmt, errors=\"coerce\"), f\"{col}: parsed with explicit format '{fmt}'\"\n",
    "\n",
    "    # 3) Try explicit timezone-aware ISO if hints exist\n",
    "    s_str = s.astype(str)\n",
    "    if s_str.str.contains(\"Z|\\\\+\\\\d{2}:?\\\\d{2}\", regex=True).any():\n",
    "        for iso_fmt in (\"%Y-%m-%dT%H:%M%z\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%dT%H:%M:%S.%f%z\"):\n",
    "            parsed = pd.to_datetime(s, format=iso_fmt, errors=\"coerce\")\n",
    "            if parsed.notna().any():\n",
    "                return parsed, f\"{col}: parsed with explicit ISO+tZ format '{iso_fmt}'\"\n",
    "\n",
    "    # 4) General fallback (dateutil)\n",
    "    parsed = pd.to_datetime(s, errors=\"coerce\")\n",
    "    audit = f\"{col}: fallback via pd.to_datetime (dateutil parser)\"\n",
    "\n",
    "    # 5) Optional dayfirst retry for ambiguous d/m\n",
    "    if dayfirst_fallback and parsed.isna().mean() > 0 and s_str.str.contains(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\", regex=True).any():\n",
    "        parsed2 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "        # Prefer the parse that yields fewer NaT\n",
    "        if parsed2.notna().sum() > parsed.notna().sum():\n",
    "            parsed, audit = parsed2, f\"{col}: fallback via pd.to_datetime (dayfirst=True)\"\n",
    "\n",
    "    return parsed, audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cedfed6-6cc2-4218-992c-2f5416c26fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- GENERIC MISSING-VALUE HANDLER (two parameters, extended) ----------\n",
    "def handle_missing(col: str, action: str):\n",
    "    \"\"\"\n",
    "    Handle missing values for a single column using a compact action string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : str\n",
    "        Column name (silently no-ops if not present).\n",
    "    action : str\n",
    "        One of:\n",
    "          - 'set_zero' | 'zero'\n",
    "          - 'set_nan'  | 'nan'\n",
    "          - 'drop_row' | 'drop'                # drop rows where col is NA\n",
    "          - 'mean' | 'median' | 'mode'         # global fill\n",
    "          - 'mean_by:<groupcol>'\n",
    "          - 'median_by:<groupcol>'\n",
    "          - 'mode_by:<groupcol>'\n",
    "          - 'percentile:<q>'                   # e.g., 'percentile:0.5' (=median)\n",
    "          - 'const:<value>'                    # e.g., 'const:0', 'const:(missing)'\n",
    "          - 'ffill' | 'bfill'\n",
    "          - 'interpolate:<method>'             # 'linear' (default), 'time', 'polynomial-2', ...\n",
    "    Notes\n",
    "    -----\n",
    "    - Numeric-only ops (mean/median/percentile/interpolate) are applied only if dtype is numeric.\n",
    "    - For 'interpolate:time', a DatetimeIndex is expected; falls back to 'linear' if absent.\n",
    "    - 'mode' works for both numeric/object. If multiple modes, first is used.\n",
    "    \"\"\"\n",
    "    global df\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "\n",
    "    a = action.strip()\n",
    "\n",
    "    # Helpers\n",
    "    def _is_num(s): return pd.api.types.is_numeric_dtype(s)\n",
    "    def _const_value(raw, series):\n",
    "        # Try to coerce const to column dtype when possible\n",
    "        raw = raw.strip()\n",
    "        if raw.lower() in {\"nan\", \"na\", \"null\"}:\n",
    "            return np.nan\n",
    "        if _is_num(series):\n",
    "            try:\n",
    "                return float(raw)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # For datetimes, try parsing\n",
    "        if pd.api.types.is_datetime64_any_dtype(series):\n",
    "            try:\n",
    "                return pd.to_datetime(raw, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return raw  # fallback: string\n",
    "\n",
    "    # Basic actions\n",
    "    a_low = a.lower()\n",
    "    if a_low in {\"set_zero\", \"zero\"}:\n",
    "        df[col] = df[col].fillna(0)\n",
    "        return\n",
    "    if a_low in {\"set_nan\", \"nan\"}:\n",
    "        df[col] = df[col].where(~df[col].isna(), np.nan)\n",
    "        return\n",
    "    if a_low in {\"drop_row\", \"drop\"}:\n",
    "        df = df[~df[col].isna()]\n",
    "        return\n",
    "    if a_low == \"mean\":\n",
    "        if _is_num(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        return\n",
    "    if a_low == \"median\":\n",
    "        if _is_num(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        return\n",
    "    if a_low == \"mode\":\n",
    "        if df[col].isna().any():\n",
    "            m = df[col].mode(dropna=True)\n",
    "            if not m.empty:\n",
    "                df[col] = df[col].fillna(m.iloc[0])\n",
    "        return\n",
    "    if a_low in {\"ffill\", \"bfill\"}:\n",
    "        df[col] = df[col].fillna(method=a_low)\n",
    "        return\n",
    "\n",
    "    # Parameterized: const:<value>\n",
    "    if a_low.startswith(\"const:\"):\n",
    "        raw = a.split(\":\", 1)[1]\n",
    "        val = _const_value(raw, df[col])\n",
    "        df[col] = df[col].fillna(val)\n",
    "        return\n",
    "\n",
    "    # Parameterized: percentile:<q>\n",
    "    if a_low.startswith(\"percentile:\"):\n",
    "        if _is_num(df[col]):\n",
    "            try:\n",
    "                q = float(a.split(\":\", 1)[1])\n",
    "                q = min(max(q, 0.0), 1.0)\n",
    "                val = df[col].quantile(q)\n",
    "                df[col] = df[col].fillna(val)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    # Parameterized: mean_by:/median_by:/mode_by:<groupcol>\n",
    "    if any(a_low.startswith(prefix) for prefix in (\"mean_by:\", \"median_by:\", \"mode_by:\")):\n",
    "        try:\n",
    "            method, grp = a_low.split(\":\", 1)\n",
    "            grp = grp.strip()\n",
    "            if grp in df.columns and df[col].isna().any():\n",
    "                if method == \"mean_by\" and _is_num(df[col]):\n",
    "                    fill_vals = df.groupby(grp)[col].transform(\"mean\")\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "                elif method == \"median_by\" and _is_num(df[col]):\n",
    "                    fill_vals = df.groupby(grp)[col].transform(\"median\")\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "                elif method == \"mode_by\":\n",
    "                    # Works for numeric or object\n",
    "                    def _grp_mode(s):\n",
    "                        m = s.mode(dropna=True)\n",
    "                        return m.iloc[0] if not m.empty else np.nan\n",
    "                    fill_vals = df.groupby(grp)[col].transform(_grp_mode)\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    # Parameterized: interpolate:<method>\n",
    "    if a_low.startswith(\"interpolate:\"):\n",
    "        method = a.split(\":\", 1)[1].strip()\n",
    "        if not method:\n",
    "            method = \"linear\"\n",
    "        if _is_num(df[col]):\n",
    "            try:\n",
    "                if method == \"time\" and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                    # Fallback if no DatetimeIndex\n",
    "                    method = \"linear\"\n",
    "                df[col] = df[col].interpolate(method=method, limit_direction=\"both\")\n",
    "            except Exception:\n",
    "                # Silent fallback: leave as is\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    # Unknown action -> no-op\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5c81e3-78ab-43aa-ace6-6613c6fb472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Date parsing audit]\n",
      " - signup_date: parsed with explicit format '%Y-%m-%d'\n",
      "Saved cleaned dataset to saas_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load ----------\n",
    "df_raw = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# ---------- Cleaning (safe & minimal) ----------\n",
    "# 0) Ensure required column(s)\n",
    "if not _has(df_raw, [\"signup_date\"]):\n",
    "    missing = [c for c in [\"signup_date\"] if c not in df_raw.columns]\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# 1) Trim strings early\n",
    "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# 1b) Normalize obvious placeholders → NaN (consistent policy)\n",
    "df = df.replace(\n",
    "    to_replace=[\"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\", \"NaN\", \"nan\", \"NULL\", \"null\", \"-\"],\n",
    "    value=np.nan\n",
    ")\n",
    "\n",
    "# 2) Parse signup_date robustly + audit (helper avoids deprecated inference)\n",
    "parsed, audit = parse_date_with_audit(df, \"signup_date\")\n",
    "df[\"signup_date\"] = parsed\n",
    "print(\"[Date parsing audit]\")\n",
    "print(\" -\", audit)\n",
    "\n",
    "# 3) Canonicalize categoricals (title-case), but keep real NaNs\n",
    "for c in [\"plan\", \"country\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\").str.title()\n",
    "        handle_missing(c, \"set_nan\")  # make policy explicit\n",
    "\n",
    "# 4) Numeric coercions (explicit; forward-compatible)\n",
    "for c in [\"monthly_churn_prob\", \"churn_month\", \"arpu\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# 5) Derive churned from churn_month\n",
    "#    Rule: churned = 1 if churn_month > 0, else 0 (0 means not churned yet).\n",
    "if \"churn_month\" in df.columns:\n",
    "    cm = pd.to_numeric(df[\"churn_month\"], errors=\"coerce\")\n",
    "    df[\"churned\"] = (cm.fillna(0) > 0).astype(int)\n",
    "else:\n",
    "    if \"churned\" in df.columns:\n",
    "        del df[\"churned\"]\n",
    "\n",
    "# 6) Final typing / validations\n",
    "if \"churn_month\" in df.columns:\n",
    "    # Keep NaNs as zero *by design* here, then clamp and cast\n",
    "    df[\"churn_month\"] = cm.fillna(0).clip(lower=0).round().astype(int)\n",
    "\n",
    "if \"monthly_churn_prob\" in df.columns:\n",
    "    df[\"monthly_churn_prob\"] = df[\"monthly_churn_prob\"].clip(lower=0)\n",
    "\n",
    "if \"arpu\" in df.columns:\n",
    "    df[\"arpu\"] = df[\"arpu\"].clip(lower=0)\n",
    "\n",
    "# 7) Drop duplicates; light numeric impute with generic handler (exclude churn_month)\n",
    "df = df.drop_duplicates()\n",
    "numeric_exclude = {\"churn_month\"}  # policy: don’t median-impute churn_month\n",
    "for c in df.select_dtypes(include=[np.number]).columns:\n",
    "    if c not in numeric_exclude and df[c].isna().any():\n",
    "        handle_missing(c, \"median\")\n",
    "\n",
    "# ---------- Save cleaned ----------\n",
    "df.to_csv(CLEAN_PATH, index=False, na_rep=\"NaN\")\n",
    "print(f\"Saved cleaned dataset to {CLEAN_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9393204-99fc-4f2a-bd66-495c0d6e78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual polish helpers (non-breaking): keep your existing _has/_normalize_freq/_nice_time_axis/_savefig/CHARTS_DIR as-is\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"axes.titleweight\": \"semibold\",\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"grid.alpha\": 0.32,\n",
    "})\n",
    "\n",
    "_PALETTE = [\n",
    "    \"#4e79a7\", \"#f28e2b\", \"#e15759\", \"#76b7b2\", \"#59a14f\",\n",
    "    \"#edc949\", \"#af7aa1\", \"#ff9da7\", \"#9c755f\", \"#bab0ab\",\n",
    "]\n",
    "\n",
    "def _colors(n: int):\n",
    "    base = list(_PALETTE)\n",
    "    return (base * ((n + len(base) - 1) // len(base)))[:n]\n",
    "\n",
    "\n",
    "def _apply_theme(ax):\n",
    "    ax.set_facecolor(\"#f9fafb\")\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "    ax.set_axisbelow(True)\n",
    "    for spine in (\"top\", \"right\"):\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "\n",
    "# ===================== Callable Plot Functions (10) =====================\n",
    "\n",
    "# 1) Signups per period (bar)\n",
    "def plot_signups_by_period(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"signup_date\",\n",
    "    freq: str = \"W\",                 # \"D\",\"W\",\"MS\",\"ME\"\n",
    "    out_dir: str = CHARTS_DIR,\n",
    "    outfile: str = \"1_signups_by_week.png\",\n",
    "    rotate_xticks: int = 45,\n",
    "    max_ticks: int = 10\n",
    "):\n",
    "    if not _has(df, [date_col]):\n",
    "        return\n",
    "    freq = _normalize_freq(freq)\n",
    "    dates = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    ok = dates.notna()\n",
    "    ts = (pd.DataFrame({date_col: dates[ok]})\n",
    "          .set_index(date_col).resample(freq).size())\n",
    "    fig, ax = plt.subplots()\n",
    "    width = 6 if freq == \"W\" else 20 if freq in (\"MS\",\"ME\") else 0.9\n",
    "    ax.bar(ts.index, ts.values, width=width, color=_colors(len(ts)), edgecolor=\"#ffffff\", linewidth=0.6)\n",
    "    label = \"Day\" if freq==\"D\" else \"Week\" if freq==\"W\" else \"Month\"\n",
    "    ax.set_title(f\"New Signups per {label}\"); ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Signups\")\n",
    "    _nice_time_axis(ax, ts.index, freq=freq, max_ticks=max_ticks, rotate=rotate_xticks)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# 2) Plan mix (bar)\n",
    "def plot_plan_mix(\n",
    "    df: pd.DataFrame,\n",
    "    plan_col: str = \"plan\",\n",
    "    out_dir: str = CHARTS_DIR,\n",
    "    outfile: str = \"2_plan_mix.png\"\n",
    "):\n",
    "    if not _has(df, [plan_col]):\n",
    "        return\n",
    "    s = df[plan_col].value_counts()\n",
    "    fig, ax = plt.subplots()\n",
    "    s.plot(kind=\"bar\", ax=ax, color=_colors(len(s)), edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"Plan Mix\"); ax.set_xlabel(\"Plan\"); ax.set_ylabel(\"Users\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# 3) Churn rate by plan (bar) — robust\n",
    "def plot_churn_rate_by_plan(\n",
    "    df: pd.DataFrame,\n",
    "    plan_col: str = \"plan\",\n",
    "    churned_col: str = \"churned\",\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"3_churn_rate_by_plan.png\",\n",
    "    min_signups: int = 1  # set >1 to hide tiny cohorts\n",
    "):\n",
    "    if plan_col not in df.columns or churned_col not in df.columns:\n",
    "        return False\n",
    "\n",
    "    # Clean/standardize\n",
    "    plans = df[plan_col].astype(str).str.strip()\n",
    "    churned = pd.to_numeric(df[churned_col], errors=\"coerce\")\n",
    "\n",
    "    # Force binary {0,1}\n",
    "    churned = churned.fillna(0).clip(0, 1).round().astype(int)\n",
    "\n",
    "    mask = plans.notna()\n",
    "    g = pd.DataFrame({plan_col: plans[mask], churned_col: churned[mask]})\n",
    "    if g.empty: \n",
    "        return False\n",
    "\n",
    "    agg = g.groupby(plan_col, dropna=True).agg(\n",
    "        churn_rate=(churned_col, \"mean\"),\n",
    "        signups=(churned_col, \"size\")\n",
    "    )\n",
    "    agg = agg[agg[\"signups\"] >= min_signups].sort_values(\"churn_rate\", ascending=False)\n",
    "    if agg.empty:\n",
    "        return False\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    agg[\"churn_rate\"].plot(kind=\"bar\", ax=ax, color=_colors(len(agg)), edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"Churn Rate by Plan\")\n",
    "    ax.set_xlabel(\"Plan\"); ax.set_ylabel(\"Churn Rate\")\n",
    "    # annotate counts\n",
    "    for i, (idx, row) in enumerate(agg.iterrows()):\n",
    "        ax.text(i, row[\"churn_rate\"] + 0.01, f\"n={row['signups']}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "    return True\n",
    "\n",
    "\n",
    "# 4) ARPU by plan (bar)\n",
    "def plot_arpu_by_plan(\n",
    "    df: pd.DataFrame,\n",
    "    plan_col: str = \"plan\",\n",
    "    arpu_col: str = \"arpu\",\n",
    "    out_dir: str = CHARTS_DIR,\n",
    "    outfile: str = \"4_arpu_by_plan.png\"\n",
    "):\n",
    "    if not _has(df, [plan_col, arpu_col]):\n",
    "        return\n",
    "    s = df.groupby(plan_col)[arpu_col].mean()\n",
    "    fig, ax = plt.subplots()\n",
    "    s.plot(kind=\"bar\", ax=ax, color=_colors(len(s)), edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"ARPU by Plan\"); ax.set_xlabel(\"Plan\"); ax.set_ylabel(\"ARPU ($)\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# 5) Retention curves by plan (months since signup; theoretical from churn prob or empirical from churn_month)\n",
    "def plot_retention_curves(\n",
    "    df: pd.DataFrame,\n",
    "    plan_col: str = \"plan\",\n",
    "    churn_prob_col: str = \"monthly_churn_prob\",\n",
    "    churn_month_col: str = \"churn_month\",\n",
    "    months: int = 12,\n",
    "    out_dir: str = CHARTS_DIR,\n",
    "    outfile: str = \"5_retention_curves.png\"\n",
    "):\n",
    "    if not _has(df, [plan_col]):\n",
    "        return\n",
    "    fig, ax = plt.subplots()\n",
    "    plotted = False\n",
    "\n",
    "    # Prefer empirical survival if churn_month present\n",
    "    if churn_month_col in df.columns:\n",
    "        for i, (plan, g) in enumerate(df.groupby(plan_col)):\n",
    "            cm = g[churn_month_col].fillna(0).clip(lower=0).astype(int).to_numpy()\n",
    "            surv = []\n",
    "            for m in range(0, months+1):\n",
    "                # survived beyond m if churn_month==0 (not churned in window) or > m\n",
    "                survived = ((cm == 0) | (cm > m)).mean()\n",
    "                surv.append(survived)\n",
    "            ax.plot(range(0, months+1), surv, label=str(plan), linewidth=2.0, marker=\"o\", markersize=3.2,\n",
    "                    color=_colors(1)[0] if len(df[plan_col].unique())==1 else _colors(len(df[plan_col].unique()))[i])\n",
    "            plotted = True\n",
    "\n",
    "    # Fallback: theoretical geometric survival from monthly churn prob\n",
    "    elif churn_prob_col in df.columns:\n",
    "        for i, (plan, g) in enumerate(df.groupby(plan_col)):\n",
    "            p = g[churn_prob_col].dropna().median()\n",
    "            surv = [(1 - p) ** m for m in range(0, months+1)]\n",
    "            ax.plot(range(0, months+1), surv, label=str(plan), linewidth=2.0, marker=\"o\", markersize=3.2,\n",
    "                    color=_colors(len(df[plan_col].nunique()))[i])\n",
    "            plotted = True\n",
    "\n",
    "    if not plotted:\n",
    "        plt.close(fig); return\n",
    "\n",
    "    ax.set_title(\"Retention Curves by Plan (0–12 months)\")\n",
    "    ax.set_xlabel(\"Months since Signup\"); ax.set_ylabel(\"Retention\")\n",
    "    ax.legend(frameon=False)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# 6) MRR timeline (month-end) — sum ARPU across active users per calendar month\n",
    "def plot_mrr_timeline(\n",
    "    df: pd.DataFrame,\n",
    "    user_col: str = \"user_id\",\n",
    "    signup_col: str = \"signup_date\",\n",
    "    churn_month_col: str = \"churn_month\",\n",
    "    arpu_col: str = \"arpu\",\n",
    "    out_dir: str = CHARTS_DIR,\n",
    "    outfile: str = \"6_mrr_timeline.png\",\n",
    "    freq: str = \"ME\",\n",
    "    rotate_xticks: int = 45,\n",
    "    max_ticks: int = 12\n",
    "):\n",
    "    if not _has(df, [user_col, signup_col, arpu_col]):\n",
    "        return\n",
    "    freq = _normalize_freq(freq)\n",
    "\n",
    "    # bounds\n",
    "    sdates = pd.to_datetime(df[signup_col], errors=\"coerce\")\n",
    "    ok = sdates.notna() & df[arpu_col].notna()\n",
    "    if ok.sum() == 0: return\n",
    "    min_month = sdates[ok].min().to_period(\"M\").to_timestamp(how=\"end\")\n",
    "    max_month = sdates[ok].max().to_period(\"M\").to_timestamp(how=\"end\")\n",
    "    # extend horizon by 12 months or observed churn horizon\n",
    "    horizon = 12\n",
    "    if churn_month_col in df.columns and df[churn_month_col].notna().any():\n",
    "        horizon = max(horizon, int(df[churn_month_col].max()))\n",
    "    max_month = max_month + pd.offsets.MonthEnd(horizon)\n",
    "\n",
    "    idx = pd.date_range(min_month, max_month, freq=\"ME\")\n",
    "    mrr = pd.Series(0.0, index=idx)\n",
    "\n",
    "    # accumulate ARPU for each active month per user\n",
    "    for _, row in df[ok].iterrows():\n",
    "        start = pd.Timestamp(row[signup_col]).to_period(\"M\").to_timestamp(how=\"end\")\n",
    "        months_active = int(row[churn_month_col]) if churn_month_col in df.columns and pd.notna(row[churn_month_col]) and row[churn_month_col] > 0 else horizon\n",
    "        end = start + pd.offsets.MonthEnd(months_active)\n",
    "        rng = pd.date_range(start, min(end, idx[-1]), freq=\"ME\")\n",
    "        mrr[rng] = mrr[rng] + float(row[arpu_col])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(mrr.index, mrr.values, linewidth=2.2, marker=\"o\", markersize=3.2)\n",
    "    ax.set_title(\"MRR Timeline\"); ax.set_xlabel(\"Month\"); ax.set_ylabel(\"MRR ($)\")\n",
    "    _nice_time_axis(ax, mrr.index, freq=freq, max_ticks=max_ticks, rotate=rotate_xticks)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# 7) Churn rate by country (bar) — robust\n",
    "def plot_country_churn_rate(\n",
    "    df: pd.DataFrame,\n",
    "    country_col: str = \"country\",\n",
    "    churned_col: str = \"churned\",\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"7_country_churn_rate.png\",\n",
    "    top_n: int = 10,\n",
    "    min_signups: int = 1\n",
    "):\n",
    "    if country_col not in df.columns or churned_col not in df.columns:\n",
    "        return False\n",
    "\n",
    "    countries = df[country_col].astype(str).str.strip().str.title()\n",
    "    churned = pd.to_numeric(df[churned_col], errors=\"coerce\").fillna(0).clip(0, 1).round().astype(int)\n",
    "\n",
    "    g = pd.DataFrame({country_col: countries, churned_col: churned})\n",
    "    g = g[g[country_col].notna()]\n",
    "    if g.empty:\n",
    "        return False\n",
    "\n",
    "    agg = g.groupby(country_col, dropna=True).agg(\n",
    "        churn_rate=(churned_col, \"mean\"),\n",
    "        signups=(churned_col, \"size\")\n",
    "    )\n",
    "    agg = agg[agg[\"signups\"] >= min_signups]\n",
    "    if agg.empty:\n",
    "        return False\n",
    "\n",
    "    # show top_n by churn_rate\n",
    "    agg = agg.sort_values([\"churn_rate\", \"signups\"], ascending=[False, False]).head(top_n)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    agg[\"churn_rate\"].plot(kind=\"bar\", ax=ax, color=_colors(len(agg)), edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"Churn Rate by Country (Top)\")\n",
    "    ax.set_xlabel(\"Country\"); ax.set_ylabel(\"Churn Rate\")\n",
    "    for i, (idx, row) in enumerate(agg.iterrows()):\n",
    "        ax.text(i, row[\"churn_rate\"] + 0.01, f\"n={row['signups']}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "    return True\n",
    "\n",
    "\n",
    "# 8) ARPU distribution (hist)\n",
    "def plot_arpu_hist(\n",
    "    df: pd.DataFrame,\n",
    "    arpu_col: str = \"arpu\",\n",
    "    out_dir: str = CHARTS_DIR,\n",
    "    outfile: str = \"8_arpu_hist.png\",\n",
    "    bins: int = 40\n",
    "):\n",
    "    if not _has(df, [arpu_col]): return\n",
    "    x = pd.to_numeric(df[arpu_col], errors=\"coerce\").dropna()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(x, bins=bins, edgecolor=\"#ffffff\", linewidth=0.6, alpha=0.95)\n",
    "    ax.set_title(\"ARPU Distribution\"); ax.set_xlabel(\"ARPU ($)\"); ax.set_ylabel(\"Users\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# 9) Tenure (months until churn) histogram — churned users only\n",
    "def plot_tenure_hist(\n",
    "    df: pd.DataFrame,\n",
    "    churn_month_col: str = \"churn_month\",\n",
    "    churned_col: str = \"churned\",\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"9_tenure_hist.png\",\n",
    "    max_bins: int = 12\n",
    "):\n",
    "    if churn_month_col not in df.columns or churned_col not in df.columns:\n",
    "        return False\n",
    "\n",
    "    churned = pd.to_numeric(df[churned_col], errors=\"coerce\").fillna(0).clip(0, 1).round().astype(int)\n",
    "    tenure = pd.to_numeric(df[churn_month_col], errors=\"coerce\")\n",
    "\n",
    "    # keep churned users with positive tenure\n",
    "    x = tenure[(churned == 1) & (tenure.notna()) & (tenure > 0)].astype(int)\n",
    "    if x.empty:\n",
    "        return False\n",
    "\n",
    "    # Nice integer bins from 1..max observed (cap to max_bins if huge)\n",
    "    m = int(x.max())\n",
    "    if m <= max_bins:\n",
    "        bins = range(1, m + 2)  # inclusive last bin\n",
    "    else:\n",
    "        bins = max_bins\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.hist(x, bins=bins, align=\"left\", rwidth=0.9, edgecolor=\"#ffffff\", linewidth=0.6, color=\"#4e79a7\")\n",
    "    ax.set_title(\"Tenure Until Churn (Months) — Churned Users\")\n",
    "    ax.set_xlabel(\"Months\"); ax.set_ylabel(\"Users\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "    return True\n",
    "\n",
    "\n",
    "# 10) LTV by plan (naive) = ARPU / median(monthly_churn_prob)\n",
    "def plot_ltv_by_plan(\n",
    "    df: pd.DataFrame,\n",
    "    plan_col: str = \"plan\",\n",
    "    arpu_col: str = \"arpu\",\n",
    "    churn_prob_col: str = \"monthly_churn_prob\",\n",
    "    out_dir: str = CHARTS_DIR,\n",
    "    outfile: str = \"10_ltv_by_plan.png\"\n",
    "):\n",
    "    if not _has(df, [plan_col, arpu_col, churn_prob_col]): return\n",
    "    agg = df.groupby(plan_col).agg(\n",
    "        arpu=(\"arpu\", \"median\"),\n",
    "        p=(\"monthly_churn_prob\", \"median\")\n",
    "    ).replace({0: np.nan})\n",
    "    agg = agg.dropna()\n",
    "    if agg.empty: return\n",
    "    agg[\"ltv\"] = agg[\"arpu\"] / agg[\"p\"]\n",
    "    fig, ax = plt.subplots()\n",
    "    agg[\"ltv\"].plot(kind=\"bar\", ax=ax, color=_colors(len(agg)), edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"Naive LTV by Plan (ARPU / churn_prob)\"); ax.set_xlabel(\"Plan\"); ax.set_ylabel(\"LTV ($)\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88bf876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Run All (with column guards, SaaS churn) =====================\n",
    "\n",
    "def _has(df, cols):\n",
    "    return all(c in df.columns for c in cols)\n",
    "\n",
    "# 1) Signups per period (weekly)\n",
    "if _has(df, [\"signup_date\"]):\n",
    "    plot_signups_by_period(\n",
    "        df, freq=\"W\", outfile=\"1_signups_by_week.png\", rotate_xticks=45, max_ticks=10\n",
    "    )\n",
    "\n",
    "# 2) Plan mix\n",
    "if _has(df, [\"plan\"]):\n",
    "    plot_plan_mix(\n",
    "        df, outfile=\"2_plan_mix.png\"\n",
    "    )\n",
    "\n",
    "# 3) Churn rate by plan (requires: plan, churned)\n",
    "if _has(df, [\"plan\", \"churned\"]):\n",
    "    plot_churn_rate_by_plan(\n",
    "        df, outfile=\"3_churn_rate_by_plan.png\", min_signups=1\n",
    "    )\n",
    "\n",
    "# 4) ARPU by plan (requires: plan, arpu)\n",
    "if _has(df, [\"plan\", \"arpu\"]):\n",
    "    plot_arpu_by_plan(\n",
    "        df, outfile=\"4_arpu_by_plan.png\"\n",
    "    )\n",
    "\n",
    "# 5) Retention curves — prefer empirical (plan + churn_month), else theoretical (plan + monthly_churn_prob)\n",
    "if _has(df, [\"plan\", \"churn_month\"]) or _has(df, [\"plan\", \"monthly_churn_prob\"]):\n",
    "    plot_retention_curves(\n",
    "        df, months=12, outfile=\"5_retention_curves.png\"\n",
    "    )\n",
    "\n",
    "# 6) MRR timeline (requires: user_id, signup_date, arpu)\n",
    "if _has(df, [\"user_id\", \"signup_date\", \"arpu\"]):\n",
    "    plot_mrr_timeline(\n",
    "        df, outfile=\"6_mrr_timeline.png\", freq=\"ME\", rotate_xticks=45, max_ticks=12\n",
    "    )\n",
    "\n",
    "# 7) Country churn rate (requires: country, churned)\n",
    "if _has(df, [\"country\", \"churned\"]):\n",
    "    plot_country_churn_rate(\n",
    "        df, outfile=\"7_country_churn_rate.png\", top_n=10, min_signups=1\n",
    "    )\n",
    "\n",
    "# 8) ARPU histogram (requires: arpu)\n",
    "if _has(df, [\"arpu\"]):\n",
    "    plot_arpu_hist(\n",
    "        df, outfile=\"8_arpu_hist.png\", bins=40\n",
    "    )\n",
    "\n",
    "# 9) Tenure histogram (requires: churn_month, churned)\n",
    "if _has(df, [\"churn_month\", \"churned\"]):\n",
    "    plot_tenure_hist(\n",
    "        df, outfile=\"9_tenure_hist.png\", max_bins=12\n",
    "    )\n",
    "\n",
    "# 10) Naive LTV by plan (requires: plan, arpu, monthly_churn_prob)\n",
    "if _has(df, [\"plan\", \"arpu\", \"monthly_churn_prob\"]):\n",
    "    plot_ltv_by_plan(\n",
    "        df, outfile=\"10_ltv_by_plan.png\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cfe65",
   "metadata": {},
   "source": [
    "\n",
    "### Delivery Notes (copy into your PDF summary)\n",
    "- **Data health**: duplicates removed; numeric nulls filled with median (customize as needed).\n",
    "- **Key patterns**: (write 3–5 bullet points referencing the charts)\n",
    "- **Outliers/Warnings**: (mention anomalies, missing ranges, unexpected spikes)\n",
    "- **Next steps**: (what simple analysis/modeling could help their decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc5c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
