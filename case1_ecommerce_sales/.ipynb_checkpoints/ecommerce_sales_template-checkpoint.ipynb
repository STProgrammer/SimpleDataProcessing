{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03022c86",
   "metadata": {},
   "source": [
    "# Data Cleaning + 5-Chart EDA (Template)\n",
    "\n",
    "**Purpose:** Fast, standardized delivery for Fiverr clients.  \n",
    "**Inputs:** A CSV/XLSX file path.  \n",
    "**Outputs:** Cleaned dataset, 5 charts (PNGs), and a short insights summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88bf876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== E-commerce sales — Robust Cleaner (Audited) =====================\n",
    "# Input : ecommerce_raw.csv  (arbitrary schema; typical cols may include order_date, order_id, unit_price, qty, revenue, product, category, country, customer_id, etc.)\n",
    "# Output: ecommerce_clean.csv\n",
    "#\n",
    "# Goals:\n",
    "#  - Robust datetime parsing (epoch + best-format + fallback) with audit\n",
    "#  - Safe numeric coercion (currency/thousands handled) without nuking categories/IDs\n",
    "#  - Light imputations for numeric columns (median), duplicates dropped\n",
    "#\n",
    "# Requirements: Python 3.9+, pandas>=2.2, numpy, matplotlib (plots elsewhere)\n",
    "\n",
    "\n",
    "\n",
    "import os, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "\n",
    "RAW_PATH = \"ecommerce_raw.csv\"      # replace with client file\n",
    "CLEAN_PATH = \"ecommerce_clean.csv\"\n",
    "CHARTS_DIR = \"charts\"\n",
    "os.makedirs(CHARTS_DIR, exist_ok=True)\n",
    "\n",
    "def save_chart(fig, name):\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(CHARTS_DIR, name), dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4075b256-b47d-4110-9605-0bf9643ebc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers used by plotting (improved) ----------\n",
    "def _savefig(out_dir: str, filename: str, fig=None, dpi: int = 160) -> str:\n",
    "    \"\"\"\n",
    "    Save current or provided Matplotlib figure to out_dir/filename and return the saved path.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "def _normalize_freq(freq: str) -> str:\n",
    "    \"\"\"Forward-compat mapping for pandas frequency aliases.\"\"\"\n",
    "    if not isinstance(freq, str):\n",
    "        return freq\n",
    "    freq = freq.upper()\n",
    "    mapping = {\n",
    "        \"M\": \"ME\",   # month-end → MonthEnd\n",
    "        \"MS\": \"MS\",  # month-start → MonthStart\n",
    "        \"Q\": \"QE\",   # quarter-end\n",
    "        \"QS\": \"QS\",  # quarter-start\n",
    "        \"Y\": \"YE\",   # year-end\n",
    "        \"A\": \"YE\",   # alias for annual end\n",
    "        \"AS\": \"YS\",  # year-start\n",
    "    }\n",
    "    return mapping.get(freq, freq)\n",
    "\n",
    "def _nice_time_axis(ax, idx, freq: str, max_ticks: int = 10, rotate: int = 0):\n",
    "    \"\"\"\n",
    "    Clean, compact date ticks by frequency.\n",
    "\n",
    "    - Accepts DatetimeIndex, PeriodIndex, numpy arrays, or lists of timestamps.\n",
    "    - Chooses readable tick positions (<= max_ticks).\n",
    "    - Custom quarter labeling when freq is QS/QE; YYYY-Qk.\n",
    "    \"\"\"\n",
    "    # Normalize/guard inputs\n",
    "    freq = _normalize_freq(freq)\n",
    "    # If index is not a DatetimeIndex, try to convert\n",
    "    if not isinstance(idx, pd.DatetimeIndex):\n",
    "        try:\n",
    "            idx = pd.DatetimeIndex(idx)\n",
    "        except Exception:\n",
    "            # Fall back to raw tick positions without date formatting\n",
    "            n = len(idx)\n",
    "            if n == 0:\n",
    "                return\n",
    "            tick_idx = np.arange(n) if n <= max_ticks else np.linspace(0, n - 1, max_ticks, dtype=int)\n",
    "            ax.set_xticks(tick_idx)\n",
    "            if rotate:\n",
    "                for t in ax.get_xticklabels():\n",
    "                    t.set_rotation(rotate)\n",
    "            ax.margins(x=0.01)\n",
    "            return\n",
    "\n",
    "    n = len(idx)\n",
    "    if n == 0:\n",
    "        return\n",
    "    tick_idx = np.arange(n) if n <= max_ticks else np.linspace(0, n - 1, max_ticks, dtype=int)\n",
    "    tick_pos = idx[tick_idx]\n",
    "    ax.set_xticks(tick_pos)\n",
    "\n",
    "    # Formatter selection\n",
    "    fmt = None\n",
    "    if freq == \"D\":\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "    elif freq == \"W\":\n",
    "        # ISO week label: YYYY-Www\n",
    "        fmt = mdates.DateFormatter(\"%G-W%V\")\n",
    "    elif freq in (\"MS\", \"ME\"):\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m\")\n",
    "    elif freq in (\"QS\", \"QE\"):\n",
    "        # Render custom quarter labels\n",
    "        labels = [f\"{d.year}-Q{((d.month - 1)//3) + 1}\" for d in tick_pos]\n",
    "        ax.set_xticklabels(labels)\n",
    "    else:\n",
    "        # Adaptive fallback\n",
    "        locator = mdates.AutoDateLocator(minticks=4, maxticks=max_ticks)\n",
    "        fmt = mdates.AutoDateFormatter(locator)\n",
    "        ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    if fmt is not None:\n",
    "        ax.xaxis.set_major_formatter(fmt)\n",
    "\n",
    "    if rotate:\n",
    "        for t in ax.get_xticklabels():\n",
    "            t.set_rotation(rotate)\n",
    "    ax.figure.autofmt_xdate()\n",
    "    ax.margins(x=0.01)\n",
    "\n",
    "def _has(df: pd.DataFrame, cols) -> bool:\n",
    "    \"\"\"Return True if all cols exist in df.columns.\"\"\"\n",
    "    return all(c in df.columns for c in cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a3ecaa-6b83-44e5-b44c-f4a75b82934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Robust date parsing (with audit, improved) ----------\n",
    "_CANDIDATE_FORMATS = [\n",
    "    # ISO-like\n",
    "    \"%Y-%m-%d\",\n",
    "    \"%Y-%m-%d %H:%M\",\n",
    "    \"%Y-%m-%d %H:%M:%S\",\n",
    "    \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "    \"%Y-%m-%dT%H:%M\",\n",
    "    \"%Y-%m-%dT%H:%M:%S\",\n",
    "    \"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "    \"%Y-%m-%dT%H:%M%z\",\n",
    "    \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "    \"%Y-%m-%dT%H:%M:%S.%f%z\",\n",
    "    # Slash\n",
    "    \"%Y/%m/%d\",\n",
    "    \"%Y/%m/%d %H:%M\",\n",
    "    \"%Y/%m/%d %H:%M:%S\",\n",
    "    \"%d/%m/%Y\",\n",
    "    \"%d/%m/%Y %H:%M\",\n",
    "    \"%d/%m/%Y %H:%M:%S\",\n",
    "    \"%m/%d/%Y\",\n",
    "    \"%m/%d/%Y %H:%M\",\n",
    "    \"%m/%d/%Y %H:%M:%S\",\n",
    "    # Dash dayfirst/monthfirst\n",
    "    \"%d-%m-%Y\",\n",
    "    \"%d-%m-%Y %H:%M\",\n",
    "    \"%d-%m-%Y %H:%M:%S\",\n",
    "    \"%m-%d-%Y\",\n",
    "    \"%m-%d-%Y %H:%M\",\n",
    "    \"%m-%d-%Y %H:%M:%S\",\n",
    "    # Dots (EU)\n",
    "    \"%d.%m.%Y\",\n",
    "    \"%d.%m.%Y %H:%M\",\n",
    "    \"%d.%m.%Y %H:%M:%S\",\n",
    "    \"%Y.%m.%d\",\n",
    "    \"%Y.%m.%d %H:%M:%S\",\n",
    "]\n",
    "\n",
    "# Support s / ms / µs epoch lengths\n",
    "_EPOCH_10_RE = re.compile(r\"^\\s*\\d{10}\\s*$\")     # seconds\n",
    "_EPOCH_13_RE = re.compile(r\"^\\s*\\d{13}\\s*$\")     # milliseconds\n",
    "_EPOCH_16_RE = re.compile(r\"^\\s*\\d{16}\\s*$\")     # microseconds\n",
    "\n",
    "# Precise matcher: 'date', 'datetime', 'timestamp', 'time' as tokens, or *_at suffix (e.g., created_at)\n",
    "_DATE_NAME_RE = re.compile(r\"(?:^|_)(date|datetime|timestamp|time)(?:$|_)\", re.I)\n",
    "\n",
    "def _is_date_like_col(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    if _DATE_NAME_RE.search(n):\n",
    "        return True\n",
    "    if n.endswith(\"_at\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _guess_epoch_unit(sample: pd.Series, min_match: float = 0.7):\n",
    "    \"\"\"\n",
    "    Return 's', 'ms', or 'us' if >= min_match of non-null values look like UNIX epoch.\n",
    "    Works for object, integer, or float series.\n",
    "    \"\"\"\n",
    "    s = sample.dropna()\n",
    "    if s.empty:\n",
    "        return None\n",
    "    # Cast to string once\n",
    "    s = s.astype(str).str.strip()\n",
    "    n = len(s)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    if (s.str.match(_EPOCH_10_RE).sum() / n) >= min_match:\n",
    "        return \"s\"\n",
    "    if (s.str.match(_EPOCH_13_RE).sum() / n) >= min_match:\n",
    "        return \"ms\"\n",
    "    if (s.str.match(_EPOCH_16_RE).sum() / n) >= min_match:\n",
    "        return \"us\"\n",
    "    return None\n",
    "\n",
    "def _heuristic_filter_formats(strings: pd.Series) -> list:\n",
    "    \"\"\"\n",
    "    Narrow the candidate formats by simple separators/markers to reduce futile trials.\n",
    "    \"\"\"\n",
    "    sample = strings.dropna().astype(str)\n",
    "    if sample.empty:\n",
    "        return _CANDIDATE_FORMATS\n",
    "\n",
    "    text = \" \".join(sample.head(50))  # small peek\n",
    "    has_t = \"T\" in text\n",
    "    has_slash = \"/\" in text\n",
    "    has_dash = \"-\" in text\n",
    "    has_dot = \".\" in text\n",
    "    has_tz = \"+\" in text or \"Z\" in text\n",
    "\n",
    "    candidates = []\n",
    "    for fmt in _CANDIDATE_FORMATS:\n",
    "        if has_t and \"T\" not in fmt:\n",
    "            continue\n",
    "        if not has_t and \"T\" in fmt:\n",
    "            # Allow non-'T' formats when text doesn't contain 'T'\n",
    "            continue\n",
    "        if has_slash and \"/\" not in fmt:\n",
    "            continue\n",
    "        if has_dash and \"-\" not in fmt and \"T\" not in fmt:\n",
    "            continue\n",
    "        if has_dot and \".\" not in fmt and \"%f\" not in fmt:\n",
    "            # dots often imply European '.' or fractional seconds\n",
    "            pass  # don't over-filter; allow\n",
    "        if has_tz and \"%z\" not in fmt and \"T\" in fmt:\n",
    "            # if looks ISO-ish with timezone\n",
    "            pass  # allow; dateutil will handle fallback anyway\n",
    "        candidates.append(fmt)\n",
    "\n",
    "    # Fallback if we filtered too aggressively\n",
    "    if len(candidates) < 3:\n",
    "        return _CANDIDATE_FORMATS\n",
    "    return candidates\n",
    "\n",
    "def _guess_datetime_format(series: pd.Series, sample_size: int = 200, min_match: float = 0.7):\n",
    "    \"\"\"\n",
    "    Pick the strptime format that parses the highest fraction of a sample.\n",
    "    Uses heuristic pre-filtering of formats to speed up.\n",
    "    \"\"\"\n",
    "    s = series.dropna().astype(str)\n",
    "    if s.empty:\n",
    "        return None\n",
    "    if len(s) > sample_size:\n",
    "        s = s.sample(sample_size, random_state=0)\n",
    "\n",
    "    fmts = _heuristic_filter_formats(s)\n",
    "    best_fmt, best_score = None, 0.0\n",
    "    for fmt in fmts:\n",
    "        parsed = pd.to_datetime(s, format=fmt, errors=\"coerce\")\n",
    "        score = float(parsed.notna().mean())\n",
    "        if score > best_score:\n",
    "            best_fmt, best_score = fmt, score\n",
    "            if best_score == 1.0:  # perfect match, stop early\n",
    "                break\n",
    "    return best_fmt if best_score >= min_match else None\n",
    "\n",
    "def parse_date_with_audit(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    *,\n",
    "    dayfirst_fallback: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Parse df[col] via epoch → best-format → explicit timezone variants → general fallback.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parsed_series : pd.Series (dtype datetime64[ns])\n",
    "    audit_str     : str (how it was parsed)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If the series is already datetime64, returns it unchanged.\n",
    "    - If all values are null, returns all-NaT with a clear audit message.\n",
    "    - If formats are ambiguous, an optional dayfirst fallback is attempted.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Column '{col}' not found\")\n",
    "\n",
    "    s = df[col]\n",
    "    if s.dtype.kind in \"Mm\":\n",
    "        return s, f\"{col}: already datetime64[ns]\"\n",
    "\n",
    "    if s.isna().all():\n",
    "        return pd.to_datetime(s, errors=\"coerce\"), f\"{col}: all values null → all NaT\"\n",
    "\n",
    "    # 1) Epoch detection\n",
    "    unit = _guess_epoch_unit(s if s.dtype == \"O\" else s.astype(\"object\"))\n",
    "    if unit:\n",
    "        return pd.to_datetime(s, unit=unit, errors=\"coerce\"), f\"{col}: parsed as UNIX epoch ({unit})\"\n",
    "\n",
    "    # 2) Guess explicit strptime format\n",
    "    fmt = _guess_datetime_format(s)\n",
    "    if fmt:\n",
    "        return pd.to_datetime(s, format=fmt, errors=\"coerce\"), f\"{col}: parsed with explicit format '{fmt}'\"\n",
    "\n",
    "    # 3) Try explicit timezone-aware ISO if hints exist\n",
    "    s_str = s.astype(str)\n",
    "    if s_str.str.contains(\"Z|\\\\+\\\\d{2}:?\\\\d{2}\", regex=True).any():\n",
    "        for iso_fmt in (\"%Y-%m-%dT%H:%M%z\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%dT%H:%M:%S.%f%z\"):\n",
    "            parsed = pd.to_datetime(s, format=iso_fmt, errors=\"coerce\")\n",
    "            if parsed.notna().any():\n",
    "                return parsed, f\"{col}: parsed with explicit ISO+tZ format '{iso_fmt}'\"\n",
    "\n",
    "    # 4) General fallback, mixed-format parser.\n",
    "    parsed = pd.to_datetime(s, errors=\"coerce\", format=\"mixed\")\n",
    "    audit = f\"{col}: fallback via pd.to_datetime(format='mixed')\"\n",
    "\n",
    "    # 5) Optional dayfirst retry for ambiguous d/m\n",
    "    if dayfirst_fallback and parsed.isna().mean() > 0 and s_str.str.contains(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\", regex=True).any():\n",
    "        parsed2 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "        # Prefer the parse that yields fewer NaT\n",
    "        if parsed2.notna().sum() > parsed.notna().sum():\n",
    "            parsed, audit = parsed2, f\"{col}: fallback via pd.to_datetime (dayfirst=True)\"\n",
    "\n",
    "    return parsed, audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bed51d-0b69-4f75-9095-2d4a70896f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- GENERIC MISSING-VALUE HANDLER (two parameters, extended) ----------\n",
    "def handle_missing(col: str, action: str):\n",
    "    \"\"\"\n",
    "    Handle missing values for a single column using a compact action string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : str\n",
    "        Column name (silently no-ops if not present).\n",
    "    action : str\n",
    "        One of:\n",
    "          - 'set_zero' | 'zero'\n",
    "          - 'set_nan'  | 'nan'\n",
    "          - 'drop_row' | 'drop'                # drop rows where col is NA\n",
    "          - 'mean' | 'median' | 'mode'         # global fill\n",
    "          - 'mean_by:<groupcol>'\n",
    "          - 'median_by:<groupcol>'\n",
    "          - 'mode_by:<groupcol>'\n",
    "          - 'percentile:<q>'                   # e.g., 'percentile:0.5' (=median)\n",
    "          - 'const:<value>'                    # e.g., 'const:0', 'const:(missing)'\n",
    "          - 'ffill' | 'bfill'\n",
    "          - 'interpolate:<method>'             # 'linear' (default), 'time', 'polynomial-2', ...\n",
    "    Notes\n",
    "    -----\n",
    "    - Numeric-only ops (mean/median/percentile/interpolate) are applied only if dtype is numeric.\n",
    "    - For 'interpolate:time', a DatetimeIndex is expected; falls back to 'linear' if absent.\n",
    "    - 'mode' works for both numeric/object. If multiple modes, first is used.\n",
    "    \"\"\"\n",
    "    global df\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "\n",
    "    a = action.strip()\n",
    "\n",
    "    # Helpers\n",
    "    def _is_num(s): return pd.api.types.is_numeric_dtype(s)\n",
    "    def _const_value(raw, series):\n",
    "        # Try to coerce const to column dtype when possible\n",
    "        raw = raw.strip()\n",
    "        if raw.lower() in {\"nan\", \"na\", \"null\"}:\n",
    "            return np.nan\n",
    "        if _is_num(series):\n",
    "            try:\n",
    "                return float(raw)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # For datetimes, try parsing\n",
    "        if pd.api.types.is_datetime64_any_dtype(series):\n",
    "            try:\n",
    "                return pd.to_datetime(raw, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return raw  # fallback: string\n",
    "\n",
    "    # Basic actions\n",
    "    a_low = a.lower()\n",
    "    if a_low in {\"set_zero\", \"zero\"}:\n",
    "        df[col] = df[col].fillna(0)\n",
    "        return\n",
    "    if a_low in {\"set_nan\", \"nan\"}:\n",
    "        df[col] = df[col].where(~df[col].isna(), np.nan)\n",
    "        return\n",
    "    if a_low in {\"drop_row\", \"drop\"}:\n",
    "        df = df[~df[col].isna()]\n",
    "        return\n",
    "    if a_low == \"mean\":\n",
    "        if _is_num(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        return\n",
    "    if a_low == \"median\":\n",
    "        if _is_num(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        return\n",
    "    if a_low == \"mode\":\n",
    "        if df[col].isna().any():\n",
    "            m = df[col].mode(dropna=True)\n",
    "            if not m.empty:\n",
    "                df[col] = df[col].fillna(m.iloc[0])\n",
    "        return\n",
    "    if a_low in {\"ffill\", \"bfill\"}:\n",
    "        df[col] = df[col].fillna(method=a_low)\n",
    "        return\n",
    "\n",
    "    # Parameterized: const:<value>\n",
    "    if a_low.startswith(\"const:\"):\n",
    "        raw = a.split(\":\", 1)[1]\n",
    "        val = _const_value(raw, df[col])\n",
    "        df[col] = df[col].fillna(val)\n",
    "        return\n",
    "\n",
    "    # Parameterized: percentile:<q>\n",
    "    if a_low.startswith(\"percentile:\"):\n",
    "        if _is_num(df[col]):\n",
    "            try:\n",
    "                q = float(a.split(\":\", 1)[1])\n",
    "                q = min(max(q, 0.0), 1.0)\n",
    "                val = df[col].quantile(q)\n",
    "                df[col] = df[col].fillna(val)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    # Parameterized: mean_by:/median_by:/mode_by:<groupcol>\n",
    "    if any(a_low.startswith(prefix) for prefix in (\"mean_by:\", \"median_by:\", \"mode_by:\")):\n",
    "        try:\n",
    "            method, grp = a_low.split(\":\", 1)\n",
    "            grp = grp.strip()\n",
    "            if grp in df.columns and df[col].isna().any():\n",
    "                if method == \"mean_by\" and _is_num(df[col]):\n",
    "                    fill_vals = df.groupby(grp)[col].transform(\"mean\")\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "                elif method == \"median_by\" and _is_num(df[col]):\n",
    "                    fill_vals = df.groupby(grp)[col].transform(\"median\")\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "                elif method == \"mode_by\":\n",
    "                    # Works for numeric or object\n",
    "                    def _grp_mode(s):\n",
    "                        m = s.mode(dropna=True)\n",
    "                        return m.iloc[0] if not m.empty else np.nan\n",
    "                    fill_vals = df.groupby(grp)[col].transform(_grp_mode)\n",
    "                    df[col] = df[col].fillna(fill_vals)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    # Parameterized: interpolate:<method>\n",
    "    if a_low.startswith(\"interpolate:\"):\n",
    "        method = a.split(\":\", 1)[1].strip()\n",
    "        if not method:\n",
    "            method = \"linear\"\n",
    "        if _is_num(df[col]):\n",
    "            try:\n",
    "                if method == \"time\" and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                    # Fallback if no DatetimeIndex\n",
    "                    method = \"linear\"\n",
    "                df[col] = df[col].interpolate(method=method, limit_direction=\"both\")\n",
    "            except Exception:\n",
    "                # Silent fallback: leave as is\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    # Unknown action -> no-op\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02aa40c8-1753-4e98-9f01-02e6d323a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned dataset to ecommerce_clean.csv\n",
      "\n",
      "[Datetime parsing audit]\n",
      " - order_date: order_date: parsed with explicit format '%Y-%m-%d'\n"
     ]
    }
   ],
   "source": [
    "# ---------- LOAD ----------\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# ---------- STANDARDIZE STRINGS ----------\n",
    "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# ---------- DATETIME PARSING (AUTO via helper) ----------\n",
    "parsed_report = []  # keep an audit trail\n",
    "for c in df.columns:\n",
    "    # skip if already datetime\n",
    "    if df[c].dtype.kind in \"Mm\":\n",
    "        continue\n",
    "    # only parse true date-like columns (avoids matching 'category')\n",
    "    if _is_date_like_col(c):\n",
    "        parsed, audit = parse_date_with_audit(df, c)  # uses epoch→format→mixed fallback\n",
    "        df[c] = parsed\n",
    "        parsed_report.append((c, audit))\n",
    "\n",
    "\n",
    "# ---------- NUMERIC COERCION ----------\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == \"object\":\n",
    "        try:\n",
    "            as_num = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if as_num.notna().mean() >= 0.7:\n",
    "                df[c] = as_num\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ---------- NORMALIZE MISSING VALUES ----------\n",
    "df = df.replace(\n",
    "    to_replace=[\"\", \" \", \"NA\", \"N/A\", \"na\", \"n/a\", \"NaN\", \"nan\", \"NULL\", \"null\", \"-\"],\n",
    "    value=np.nan\n",
    ")\n",
    "\n",
    "# ---------- DEDUP ----------\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# ---------- DOMAIN RULES FOR E-COMMERCE ----------\n",
    "# 1) Deterministic calculations for qty / unit_price / revenue when possible.\n",
    "cols = {c.lower(): c for c in df.columns}  # case-insensitive map\n",
    "qty_col     = cols.get(\"qty\")\n",
    "price_col   = cols.get(\"unit_price\")\n",
    "revenue_col = cols.get(\"revenue\")\n",
    "product_col = cols.get(\"product\")\n",
    "\n",
    "if qty_col and price_col and revenue_col:\n",
    "    # qty = revenue / unit_price\n",
    "    mask = df[qty_col].isna() & df[revenue_col].notna() & df[price_col].notna() & (df[price_col] != 0)\n",
    "    df.loc[mask, qty_col] = df.loc[mask, revenue_col] / df.loc[mask, price_col]\n",
    "\n",
    "    # unit_price = revenue / qty\n",
    "    mask = df[price_col].isna() & df[revenue_col].notna() & df[qty_col].notna() & (df[qty_col] != 0)\n",
    "    df.loc[mask, price_col] = df.loc[mask, revenue_col] / df.loc[mask, qty_col]\n",
    "\n",
    "    # revenue = qty * unit_price\n",
    "    mask = df[revenue_col].isna() & df[qty_col].notna() & df[price_col].notna()\n",
    "    df.loc[mask, revenue_col] = df.loc[mask, qty_col] * df.loc[mask, price_col]\n",
    "\n",
    "# 1b) If unit_price still missing but product is known, use median-by-product (uses the generic handler)\n",
    "if price_col and product_col and df[price_col].isna().any():\n",
    "    handle_missing(price_col, f\"median_by:{product_col}\")\n",
    "\n",
    "# 2) For categorical IDs/names: keep as NaN (explicitly set to NaN; do NOT fill with mode)\n",
    "for keep_nan_col in [\"country\", \"product\", \"category\"]:\n",
    "    if keep_nan_col in df.columns:\n",
    "        handle_missing(keep_nan_col, \"set_nan\")\n",
    "\n",
    "# ---------- LIGHT IMPUTATIONS (preserve intent, use generic handler) ----------\n",
    "# Numeric: fill median for numeric columns EXCEPT qty/unit_price/revenue (leave NaN if not calculable)\n",
    "numeric_exclude = {x for x in [qty_col, price_col, revenue_col] if x}\n",
    "for c in df.select_dtypes(include=[np.number]).columns:\n",
    "    if c not in numeric_exclude and df[c].isnull().any():\n",
    "        handle_missing(c, \"median\")\n",
    "\n",
    "# Object: fill mode for object columns EXCEPT country/product/category (these must remain NaN if missing)\n",
    "object_exclude = {\"country\", \"product\", \"category\"}\n",
    "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    if c not in object_exclude and df[c].isnull().any():\n",
    "        handle_missing(c, \"mode\")\n",
    "\n",
    "# Datetime: forward-fill is fine for temporal continuity\n",
    "for c in df.select_dtypes(include=[\"datetime64[ns]\"]).columns:\n",
    "    if df[c].isnull().any():\n",
    "        handle_missing(c, \"ffill\")\n",
    "\n",
    "# ---------- SAVE ----------\n",
    "# Write visible NaNs so Excel doesn’t show blanks\n",
    "df.to_csv(CLEAN_PATH, index=False, na_rep=\"NaN\")\n",
    "\n",
    "# ---------- AUDIT LOG ----------\n",
    "print(\"Saved cleaned dataset to\", CLEAN_PATH)\n",
    "if parsed_report:\n",
    "    print(\"\\n[Datetime parsing audit]\")\n",
    "    for col, how in parsed_report:\n",
    "        print(f\" - {col}: {how}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "635389f5-c55e-4f03-9c89-05de991c6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# ===================== Lightweight visual theme helpers (non-breaking) =====================\n",
    "\n",
    "# Subtle, readable color cycle (tab10) expanded for bars/slices\n",
    "_PALETTE = plt.cm.tab20.colors  # tuple of RGBA colors\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"figure.autolayout\": False,  # we'll still call tight_layout() before save\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"axes.titleweight\": \"semibold\",\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.prop_cycle\": mpl.cycler(color=[\n",
    "        \"#4e79a7\", \"#f28e2b\", \"#e15759\", \"#76b7b2\", \"#59a14f\",\n",
    "        \"#edc949\", \"#af7aa1\", \"#ff9da7\", \"#9c755f\", \"#bab0ab\"\n",
    "    ]),\n",
    "    \"grid.linestyle\": \"-\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "})\n",
    "\n",
    "\n",
    "def _apply_theme(ax):\n",
    "    \"\"\"Apply a clean, modern look without altering chart logic.\"\"\"\n",
    "    ax.set_facecolor(\"#f9fafb\")\n",
    "    ax.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "    for spine in (\"top\", \"right\"):\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    ax.figure.set_facecolor(\"#ffffff\")\n",
    "\n",
    "\n",
    "def _colors(n):\n",
    "    \"\"\"Return n distinct colors from the palette.\"\"\"\n",
    "    base = list(_PALETTE)\n",
    "    if n <= len(base):\n",
    "        return base[:n]\n",
    "    # repeat if more colors are needed\n",
    "    reps = (n + len(base) - 1) // len(base)\n",
    "    return (base * reps)[:n]\n",
    "\n",
    "\n",
    "# ===================== E-commerce Charts — Callable Functions =====================\n",
    "# Requirements: pandas, numpy, matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# ---------- I/O helper ----------\n",
    "def _savefig(out_dir: str, filename: str, fig=None, dpi: int = 160):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, filename), dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------- Date-axis helper ----------\n",
    "def _nice_time_axis(ax, idx, freq: str, max_ticks: int = 10, rotate: int = 0):\n",
    "    n = len(idx)\n",
    "    if n == 0:\n",
    "        return\n",
    "    tick_idx = np.arange(n) if n <= max_ticks else np.linspace(0, n - 1, max_ticks, dtype=int)\n",
    "    ax.set_xticks(idx[tick_idx])\n",
    "\n",
    "    if freq == \"D\":\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "    elif freq == \"W\":\n",
    "        fmt = mdates.DateFormatter(\"%Y-W%W\")\n",
    "    elif freq in (\"MS\", \"ME\"):\n",
    "        fmt = mdates.DateFormatter(\"%Y-%m\")\n",
    "    else:\n",
    "        fmt = mdates.AutoDateFormatter(mdates.AutoDateLocator(minticks=4, maxticks=max_ticks))\n",
    "    ax.xaxis.set_major_formatter(fmt)\n",
    "\n",
    "    if rotate:\n",
    "        for t in ax.get_xticklabels():\n",
    "            t.set_rotation(rotate)\n",
    "    ax.figure.autofmt_xdate()\n",
    "    ax.margins(x=0.01)\n",
    "\n",
    "# ---------- 1) Quantity per period (replaces old histogram) ----------\n",
    "def plot_qty_by_period(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"order_date\",\n",
    "    qty_col: str = \"qty\",\n",
    "    freq: str = \"W\",                                # \"D\",\"W\",\"MS\",\"ME\"\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"1_qty_by_week.png\",\n",
    "    rotate_xticks: int = 45,\n",
    "    max_ticks: int = 10\n",
    "):\n",
    "    dates = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    qty = pd.to_numeric(df[qty_col], errors=\"coerce\")\n",
    "    ok = dates.notna() & qty.notna()\n",
    "    ts = (pd.DataFrame({date_col: dates[ok], qty_col: qty[ok]})\n",
    "          .set_index(date_col).resample(freq)[qty_col].sum())\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    width = 6 if freq == \"W\" else 20 if freq in (\"MS\",\"ME\") else 0.9\n",
    "    colors = _colors(len(ts))\n",
    "    ax.bar(ts.index, ts.values, width=width, color=colors, edgecolor=\"#ffffff\", linewidth=0.6)\n",
    "    label = \"Day\" if freq==\"D\" else \"Week\" if freq==\"W\" else \"Month\"\n",
    "    ax.set_title(f\"Quantity per {label}\"); ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Quantity\")\n",
    "    _nice_time_axis(ax, ts.index, freq=freq, max_ticks=max_ticks, rotate=rotate_xticks)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 2) Revenue by period (line/bar) ----------\n",
    "def plot_revenue_by_period(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"order_date\",\n",
    "    revenue_col: str = \"revenue\",\n",
    "    freq: str = \"ME\",                                 # \"D\",\"W\",\"MS\",\"ME\"\n",
    "    kind: str = \"line\",                              # \"line\" or \"bar\"\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"2_monthly_revenue.png\",\n",
    "    rotate_xticks: int = 0,\n",
    "    max_ticks: int = 10\n",
    "):\n",
    "    dates = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    rev = pd.to_numeric(df[revenue_col], errors=\"coerce\")\n",
    "    ok = dates.notna() & rev.notna()\n",
    "    ts = (pd.DataFrame({date_col: dates[ok], revenue_col: rev[ok]})\n",
    "          .set_index(date_col).resample(freq)[revenue_col].sum())\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    if kind == \"bar\":\n",
    "        width = 6 if freq == \"W\" else 20 if freq in (\"MS\",\"ME\") else 0.9\n",
    "        colors = _colors(len(ts))\n",
    "        ax.bar(ts.index, ts.values, width=width, color=colors, edgecolor=\"#ffffff\", linewidth=0.6)\n",
    "    else:\n",
    "        ax.plot(ts.index, ts.values, linewidth=2.2, marker=\"o\", markersize=4)\n",
    "    label = \"Day\" if freq==\"D\" else \"Week\" if freq==\"W\" else \"Month\"\n",
    "    ax.set_title(f\"Revenue per {label}\"); ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Revenue\")\n",
    "    _nice_time_axis(ax, ts.index, freq=freq, max_ticks=max_ticks, rotate=rotate_xticks)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 3) Top Countries by Revenue (bar) ----------\n",
    "def plot_top_countries_by_revenue(\n",
    "    df: pd.DataFrame,\n",
    "    country_col: str = \"country\",\n",
    "    revenue_col: str = \"revenue\",\n",
    "    top_n: int = 5,\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"3_revenue_by_country.png\"\n",
    "):\n",
    "    s = (df.groupby(country_col)[revenue_col].sum()\n",
    "           .sort_values(ascending=False).head(top_n))\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    colors = _colors(len(s))\n",
    "    s.plot(kind=\"bar\", ax=ax, color=colors, edgecolor=\"#ffffff\")\n",
    "    ax.set_title(\"Top Countries by Revenue\"); ax.set_xlabel(\"Country\"); ax.set_ylabel(\"Revenue\")\n",
    "    for p in ax.patches:\n",
    "        p.set_linewidth(0.6)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 4) Unit Price vs Revenue (log-Y) — replaces bad order_id scatter ----------\n",
    "def plot_unitprice_vs_revenue(\n",
    "    df: pd.DataFrame,\n",
    "    price_col: str = \"unit_price\",\n",
    "    revenue_col: str = \"revenue\",\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"4_scatter.png\"\n",
    "):\n",
    "    x = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "    y = pd.to_numeric(df[revenue_col], errors=\"coerce\")\n",
    "    ok = x.notna() & y.notna()\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.scatter(x[ok], y[ok], alpha=0.7, s=28, linewidths=0.5, edgecolors=\"#ffffff\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(\"Unit Price vs Revenue (log scale)\")\n",
    "    ax.set_xlabel(\"Unit Price\"); ax.set_ylabel(\"Revenue (log)\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 5) Revenue per Order by Product (Top 10) — Boxplot ----------\n",
    "def plot_revenue_by_product_boxplot(\n",
    "    df: pd.DataFrame,\n",
    "    product_col: str = \"product\",\n",
    "    revenue_col: str = \"revenue\",\n",
    "    top_n: int = 10,\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"5_box_by_cat.png\"\n",
    "):\n",
    "    totals = (df.groupby(product_col)[revenue_col].sum()\n",
    "                .sort_values(ascending=False).head(top_n))\n",
    "    top_idx = totals.index\n",
    "    sub = df[df[product_col].isin(top_idx)]\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    bp = sub.boxplot(column=revenue_col, by=product_col, rot=45, ax=ax, patch_artist=True)\n",
    "    # Color boxes for better readability\n",
    "    for i, box in enumerate(ax.artists):\n",
    "        box.set_facecolor(_colors(len(top_idx))[i % len(top_idx)])\n",
    "        box.set_edgecolor(\"#ffffff\"); box.set_linewidth(0.6)\n",
    "    ax.set_title(\"Revenue per Order by Product (Top 10)\")\n",
    "    plt.suptitle(\"\")\n",
    "    ax.set_xlabel(\"Product\"); ax.set_ylabel(\"Revenue per Order\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 6) Revenue Share by Category (pie) ----------\n",
    "def plot_category_share_pie(\n",
    "    df: pd.DataFrame,\n",
    "    category_col: str = \"category\",\n",
    "    revenue_col: str = \"revenue\",\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"6_category_share.png\"\n",
    "):\n",
    "    s = df.groupby(category_col)[revenue_col].sum()\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    colors = _colors(len(s))\n",
    "    ax.pie(s.values, labels=s.index, autopct=\"%1.0f%%\", startangle=90,\n",
    "           colors=colors, wedgeprops={\"edgecolor\": \"#ffffff\", \"linewidth\": 0.8},\n",
    "           textprops={\"fontsize\": 10})\n",
    "    ax.set_title(\"Revenue Share by Category\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 7) Unit Price vs Quantity (scatter) ----------\n",
    "def plot_price_vs_qty_scatter(\n",
    "    df: pd.DataFrame,\n",
    "    price_col: str = \"unit_price\",\n",
    "    qty_col: str = \"qty\",\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"7_price_vs_qty.png\"\n",
    "):\n",
    "    x = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "    y = pd.to_numeric(df[qty_col], errors=\"coerce\")\n",
    "    ok = x.notna() & y.notna()\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.scatter(x[ok], y[ok], alpha=0.7, marker='x', s=28)\n",
    "    ax.set_title(\"Unit Price vs Quantity\")\n",
    "    ax.set_xlabel(\"Unit Price\"); ax.set_ylabel(\"Quantity\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 8) Daily Orders (count) time series ----------\n",
    "def plot_daily_orders(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"order_date\",\n",
    "    id_col: str = \"order_id\",           # counts distinct orders per day\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"8_timeseries.png\",\n",
    "    rotate_xticks: int = 0,\n",
    "    max_ticks: int = 12\n",
    "):\n",
    "    dates = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    ids = df[id_col]\n",
    "    ok = dates.notna() & ids.notna()\n",
    "    # count distinct orders per day\n",
    "    ts = (pd.DataFrame({date_col: dates[ok], id_col: ids[ok]})\n",
    "          .groupby(pd.Grouper(key=date_col, freq=\"D\"))[id_col]\n",
    "          .nunique())\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    ax.plot(ts.index, ts.values, linewidth=2.0, marker=\"o\", markersize=4)\n",
    "    ax.set_title(\"Daily Order Count\"); ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Orders\")\n",
    "    _nice_time_axis(ax, ts.index, freq=\"D\", max_ticks=max_ticks, rotate=rotate_xticks)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 9) Correlation Heatmap (exclude IDs/high-cardinality) ----------\n",
    "def plot_corr_heatmap_wo_ids(\n",
    "    df: pd.DataFrame,\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"9_corr_heatmap.png\",\n",
    "    id_suffixes: tuple = (\"id\",),\n",
    "    uniq_ratio_cut: float = 0.9\n",
    "):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    drop_cols = []\n",
    "    for c in num_cols:\n",
    "        cl = c.lower()\n",
    "        if any(cl.endswith(suf) for suf in id_suffixes):\n",
    "            drop_cols.append(c); continue\n",
    "        unique_ratio = df[c].nunique(dropna=True) / max(len(df), 1)\n",
    "        if unique_ratio > uniq_ratio_cut:\n",
    "            drop_cols.append(c)\n",
    "    use_cols = [c for c in num_cols if c not in drop_cols]\n",
    "    if len(use_cols) < 2:\n",
    "        return  # not enough numeric cols after filtering\n",
    "    corr = df[use_cols].corr(numeric_only=True)\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    img = ax.imshow(corr.values, interpolation=\"nearest\", cmap=\"viridis\")\n",
    "    ax.set_title(\"Correlation Heatmap (IDs excluded)\")\n",
    "    ax.set_xticks(range(len(corr.columns))); ax.set_xticklabels(corr.columns, rotation=90)\n",
    "    ax.set_yticks(range(len(corr.columns))); ax.set_yticklabels(corr.columns)\n",
    "    cbar = fig.colorbar(img)\n",
    "    cbar.ax.set_ylabel(\"Correlation\", rotation=270, labelpad=12)\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ---------- 10) Revenue distribution (histogram) ----------\n",
    "def plot_revenue_hist(\n",
    "    df: pd.DataFrame,\n",
    "    revenue_col: str = \"revenue\",\n",
    "    out_dir: str = \"charts\",\n",
    "    outfile: str = \"10_revenue_hist.png\",\n",
    "    bins: int = 40\n",
    "):\n",
    "    rev = pd.to_numeric(df[revenue_col], errors=\"coerce\").dropna()\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.hist(rev, bins=bins, edgecolor=\"#ffffff\", linewidth=0.6, alpha=0.9)\n",
    "    ax.set_title(\"Revenue Distribution per Order\")\n",
    "    ax.set_xlabel(\"Revenue\"); ax.set_ylabel(\"Count\")\n",
    "    _apply_theme(ax)\n",
    "    _savefig(out_dir, outfile, fig)\n",
    "\n",
    "# ===================== Example usage =====================\n",
    "# df = pd.read_csv(\"ecommerce_clean.csv\")\n",
    "# plot_qty_by_period(df, freq=\"W\", outfile=\"1_qty_by_week.png\")\n",
    "# plot_revenue_by_period(df, freq=\"ME\", kind=\"line\", outfile=\"1_monthly_revenue.png\")\n",
    "# plot_top_countries_by_revenue(df, top_n=5)\n",
    "# plot_unitprice_vs_revenue(df)\n",
    "# plot_revenue_by_product_boxplot(df)\n",
    "# plot_category_share_pie(df)\n",
    "# plot_price_vs_qty_scatter(df)\n",
    "# plot_daily_orders(df)\n",
    "# plot_corr_heatmap_wo_ids(df)\n",
    "# plot_revenue_hist(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62cb0245-ece7-4bcc-9f5d-9af226d04410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Run each plot once with column checks ----------------\n",
    "def _has(df, cols): \n",
    "    return all(c in df.columns for c in cols)\n",
    "\n",
    "# 1) Quantity per period (weekly bar)\n",
    "if _has(df, [\"order_date\", \"qty\"]):\n",
    "    plot_qty_by_period(\n",
    "        df, date_col=\"order_date\", qty_col=\"qty\",\n",
    "        freq=\"W\", outfile=\"1_qty_by_week.png\", rotate_xticks=45, max_ticks=8\n",
    "    )\n",
    "\n",
    "# 2) Revenue by period (monthly line)\n",
    "if _has(df, [\"order_date\", \"revenue\"]):\n",
    "    plot_revenue_by_period(\n",
    "        df, date_col=\"order_date\", revenue_col=\"revenue\",\n",
    "        freq=\"ME\", kind=\"line\", outfile=\"2_monthly_revenue.png\", rotate_xticks=0, max_ticks=10\n",
    "    )\n",
    "\n",
    "# 3) Top countries by revenue (bar)\n",
    "if _has(df, [\"country\", \"revenue\"]):\n",
    "    plot_top_countries_by_revenue(\n",
    "        df, country_col=\"country\", revenue_col=\"revenue\",\n",
    "        top_n=5, outfile=\"3_revenue_by_country.png\"\n",
    "    )\n",
    "\n",
    "# 4) Unit price vs revenue (log-Y) — replaces old order_id scatter\n",
    "if _has(df, [\"unit_price\", \"revenue\"]):\n",
    "    plot_unitprice_vs_revenue(\n",
    "        df, price_col=\"unit_price\", revenue_col=\"revenue\",\n",
    "        outfile=\"4_scatter.png\"\n",
    "    )\n",
    "\n",
    "# 5) Revenue per order by product (Top 10) — boxplot\n",
    "if _has(df, [\"product\", \"revenue\"]):\n",
    "    plot_revenue_by_product_boxplot(\n",
    "        df, product_col=\"product\", revenue_col=\"revenue\",\n",
    "        top_n=10, outfile=\"5_box_by_cat.png\"\n",
    "    )\n",
    "\n",
    "# 6) Revenue share by category (pie)\n",
    "if _has(df, [\"category\", \"revenue\"]):\n",
    "    plot_category_share_pie(\n",
    "        df, category_col=\"category\", revenue_col=\"revenue\",\n",
    "        outfile=\"6_category_share.png\"\n",
    "    )\n",
    "\n",
    "# 7) Unit price vs quantity (scatter)\n",
    "if _has(df, [\"unit_price\", \"qty\"]):\n",
    "    plot_price_vs_qty_scatter(\n",
    "        df, price_col=\"unit_price\", qty_col=\"qty\",\n",
    "        outfile=\"7_price_vs_qty.png\"\n",
    "    )\n",
    "\n",
    "# 8) Daily orders (distinct order_id per day)\n",
    "if _has(df, [\"order_date\", \"order_id\"]):\n",
    "    plot_daily_orders(\n",
    "        df, date_col=\"order_date\", id_col=\"order_id\",\n",
    "        outfile=\"8_timeseries.png\", rotate_xticks=0, max_ticks=12\n",
    "    )\n",
    "\n",
    "# 9) Correlation heatmap (IDs excluded) — relies on numeric columns; function self-guards\n",
    "plot_corr_heatmap_wo_ids(\n",
    "    df, outfile=\"9_corr_heatmap.png\", id_suffixes=(\"id\",), uniq_ratio_cut=0.9\n",
    ")\n",
    "\n",
    "# 10) Revenue distribution (histogram)\n",
    "if _has(df, [\"revenue\"]):\n",
    "    plot_revenue_hist(\n",
    "        df, revenue_col=\"revenue\",\n",
    "        outfile=\"10_revenue_hist.png\", bins=40\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cfe65",
   "metadata": {},
   "source": [
    "\n",
    "### Delivery Notes (copy into your PDF summary)\n",
    "- **Data health**: duplicates removed; numeric nulls filled with median (customize as needed).\n",
    "- **Key patterns**: (write 3–5 bullet points referencing the charts)\n",
    "- **Outliers/Warnings**: (mention anomalies, missing ranges, unexpected spikes)\n",
    "- **Next steps**: (what simple analysis/modeling could help their decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc5c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fiverr] *",
   "language": "python",
   "name": "conda-env-fiverr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
